{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-04T02:57:22.430061Z",
     "start_time": "2025-02-04T02:57:20.995784Z"
    }
   },
   "source": [
    "import torch\n",
    "from numpy import dtype\n",
    "from sympy.codegen.ast import float32\n",
    "\n",
    "x = torch.rand(3)\n",
    "print(x)\n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "x = torch.empty(3)\n",
    "print(x)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1361, 0.9594, 0.1303])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x = torch.empty(2, 3)\n",
    "print(x)"
   ],
   "id": "ca7aaaf5d1731352"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x = torch.empty(2, 3, 2)\n",
    "print(x)"
   ],
   "id": "25599daaede69323"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x = torch.empty(2, 3, 2, 2)\n",
    "print(x)"
   ],
   "id": "503090d1d04da03a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x = torch.rand(2,2)\n",
    "print(x)"
   ],
   "id": "30df6d240acb7a48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x = torch.zeros(2,2)\n",
    "print(x)"
   ],
   "id": "68d786a4fe2ff35d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x = torch.ones(2,2)\n",
    "print(x)"
   ],
   "id": "d5f3adff041544c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x = torch.ones(2,2)\n",
    "print(x.dtype)"
   ],
   "id": "f4e1a70e2582fdc9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x = torch.ones(2,2, dtype=torch.int) #torch.double, torch.float16, torch.float64\n",
    "print(x.dtype) #prints the dtype of the current tensor"
   ],
   "id": "f05cf91f7f4cd89b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x = torch.ones(2,2, dtype=torch.float16)\n",
    "print(x.size()) # prints the size of the current tensor"
   ],
   "id": "bde25427470147e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x = torch.tensor([2.5, 0.1])\n",
    "print(x)"
   ],
   "id": "fc07c30934b2dc32"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "BASIC OPERATIONS",
   "id": "2556f15d79d1c98f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x = torch.rand(2,2)\n",
    "y = torch.rand(2,2)\n",
    "print(x)\n",
    "print(y)\n",
    "z = x + y\n",
    "z = torch.add(x,y)\n",
    "print(z) # it does element wise addition\n"
   ],
   "id": "b9744c58621c7d24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "y.add_(x) # inplace operation. Add all of the x's to the y. It modifies the variable in which it is applied to\n",
    "print(y)"
   ],
   "id": "e4eb8fca20db4f9d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "z = x - y\n",
    "z = torch.sub(x,y)\n",
    "print(z)"
   ],
   "id": "da9ab11c3c3b5236"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "z = x * y\n",
    "z = torch.mul(x,y)\n",
    "print(z)\n",
    "y.mul_(x) #inplace operation"
   ],
   "id": "89abdf50080bf0f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "z = x / y\n",
    "z = torch.div(x,y)"
   ],
   "id": "51256b0dee34b3cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#slciing operations\n",
    "x = torch.rand(5,3)\n",
    "print(x[:,0]) #slicing for all rows but only the first column of each row\n",
    "print(x[1, :]) #slicing for only the first row but all of the columns from said row\n",
    "#you can also get the value of a tensor, if and only if is a single element tensor\n",
    "print(x[1, 1].item())\n"
   ],
   "id": "d8b65094524e5c0b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# reshaping a tensor\n",
    "x = torch.rand(4,4)\n",
    "print(x)\n",
    "y = x.view(16) #resizes the 16 values in a 4x4 tensor and a single 1x16. NOTE: the number of elements must match\n",
    "print(y)\n",
    "\n",
    "y = x.view(-1, 8) #if we don't want to resize to a single dimension tensor, then we can put a -1 on the first position and pytorch will correctly assume the dimension for the number of rows. i.e. 2x8\n",
    "print(y.size)"
   ],
   "id": "3b7a34c76b436bbb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:24:49.119830Z",
     "start_time": "2025-02-04T03:24:49.112657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#converting from numpy to tensor and viceversa\n",
    "import numpy as np\n",
    "# convert from a tensor into a np array\n",
    "a = torch.ones(5)\n",
    "print(a)\n",
    "b = a.numpy()\n",
    "print(type(b))  #i.e. numpy ndarray\n",
    "\n",
    "#if the tensor is in the cpu, then both objects(tensor and nparray) will share the same memory location, so any change to either will reflect on the other\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ],
   "id": "8d07a966d0a43a04",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "<class 'numpy.ndarray'>\n",
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:26:30.273950Z",
     "start_time": "2025-02-04T03:26:30.267841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#convert from a numpy array to a tensor\n",
    "a = np.ones(5)\n",
    "print(a)\n",
    "b = torch.from_numpy(a) #be careful changing the dtype, both must match. Otherwise you get an error\n",
    "print(b)\n",
    "\n",
    "a += 1\n",
    "print(a)\n",
    "print(b)\n"
   ],
   "id": "2ac57e2f9e1e065b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:31:13.450574Z",
     "start_time": "2025-02-04T03:31:13.436679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#gpu operations\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    x = torch.ones(5, device=device) #creates a tensor in the GPU \n",
    "    y = torch.ones(5) \n",
    "    y = y.to(device) # this moves the tensor to the GPU\n",
    "    z = x + y # this will be computed in the GPU\n",
    "    # you can't convert a GPU tensor into a numpy ndarray, so you must first move it into the cpu(numpy can only handle cpu tensors)\n",
    "    z = z.to(\"cpu\")\n",
    "    print(z)"
   ],
   "id": "b6fb73da74848bf7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "x = torch.ones(5, requires_grad=True) #the requires_grad flag is FALSE by default. It tells pytorch that it will need to calculate the gradients for this tensor later in the optimization steps. \n",
    "print(x) # this will also print the requires_grad flag as well"
   ],
   "id": "5f0ff1aa6e5b7e86"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Gradient Calculations with autograd",
   "id": "d9e7f31a380fc5d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:38:54.574281Z",
     "start_time": "2025-02-04T03:38:54.567676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#gradients are essential for optimization\n",
    "import torch \n",
    "\n",
    "x = torch.rand(3, requires_grad=True) # must specify the requires_grad to calculate the gradients\n",
    "print(x)\n",
    "y = x + 2 # pytorch creates a computational graph\n",
    "# pytorch will compute the forward pass, and create a grad_fn function and calculate the gradients during the backpropagation step(dy/dx)\n",
    "print(y) #has a grad_fn for AddBackward\n",
    "z = y*y*2\n",
    "print(z) #has a grad_fn for MulBackward\n",
    "z = z.mean()\n",
    "print(z) # has a grad_fn for MeanBackward\n",
    "\n",
    "# now when we calculate the gradients for z \n",
    "z.backward() #backward gradient calculation  dz/dx. The backward function will only work with scalar values(single value), if called without an argument\n",
    "print(x.grad) # the gradients are stored in x.grad\n",
    "#NOTE if we don't specify the requires_grad, then we won't have a .backward() grad_fn, giving us an error\n",
    " \n",
    "\n"
   ],
   "id": "91dbf909bca1c55b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5928, 0.9772, 0.1487], requires_grad=True)\n",
      "tensor([2.5928, 2.9772, 2.1487], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The gradient calculation calculates a Jacobian product to get the gradients. We multiply the jacobian matrix containing the partial derivatives with a gradient vector(of the same size )",
   "id": "1494434079cb57b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.001], dtype=torch.float32)\n",
    "z.backward(v) #dz/dx\n",
    "print(x.grad)"
   ],
   "id": "270eecacae975a00"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:59:33.534715Z",
     "start_time": "2025-02-04T03:59:33.529262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# sometimes during our training, when we want update the weight, we don't want pytorch to calculate the gradients\n",
    "# we have 3 options for this\n",
    "\n",
    "# x.requires_grad_(False)\n",
    "# x.detach()\n",
    "# with torch.no_grad()\n",
    "\n",
    "x.requires_grad_(False) # the underscore means that pytorch will modify the variable in place\n",
    "print(x)\n",
    "\n",
    "y = x.detach() # creates a new tensor with the same values but it doesn't requrie the gradients (so requires_grad = False)\n",
    "print(y)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = x + 2\n",
    "    print(z)"
   ],
   "id": "693ea07e74630dff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5928, 0.9772, 0.1487])\n",
      "tensor([0.5928, 0.9772, 0.1487])\n",
      "tensor([2.5928, 2.9772, 2.1487])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T04:03:32.997796Z",
     "start_time": "2025-02-04T04:03:32.993221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# VERY IMPORTANT: Whenever we call the backward function, then the gradient for the tensor will be accumulated into the .grad attribute, so the values will be summed up\n",
    "weights = torch.ones(4, requires_grad=True)\n",
    "for epoch in range(3):\n",
    "    model_output = (weights*3).sum()\n",
    "    \n",
    "    model_output.backward()\n",
    "    \n",
    "    print(weights.grad) #gradients get aggregated [[3...],[6....],[9.....]]\n",
    "    weights.grad.zero_() #you must empty the gradients after each iteration to get the correct gradients [3....]\n",
    "    \n",
    "# same thing but with an optimizer\n",
    "optimizer = torch.optim.SGD(weights, lr=0.01)\n",
    "optimizer.step() # next iteration\n",
    "optimizer.zero_grad() # empty the gradients "
   ],
   "id": "226ece1ae8d3919e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "BACKPROPAGATION - THEORY WITH EXAMPLES",
   "id": "1ea655f22661c3c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T04:21:10.021879Z",
     "start_time": "2025-02-04T04:21:10.013762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "#forward pass and compute the loss\n",
    "y_hat = w * x\n",
    "loss = (y_hat - y)**2\n",
    "print(loss)\n",
    "\n",
    "#backward pass\n",
    "loss.backward() #calculates the gradients during the backward pass and stores them in the tensors that have requires_grad \n",
    "print(w.grad) #prints the stored gradients for w\n",
    "\n",
    "##update weights\n",
    "## next forward and backward pass"
   ],
   "id": "bd08716d50bb05f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n",
      "tensor(-2.)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "GRADIENT DESCENT WITH AUTOGRAD AND BACKPROPAGATION",
   "id": "7917e2fd855354f3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T21:35:04.971891Z",
     "start_time": "2025-02-04T21:35:04.965750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# f = w * x\n",
    "# f = 2 * x\n",
    "x = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
    "\n",
    "w = 0.0\n",
    "\n",
    "# calculate the model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "# calculate the loss = MSE\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted - y) ** 2).mean()\n",
    "# calculate the gradient\n",
    "# MSE = 1/N * (w*x - y)**2\n",
    "# dJ/dw = 1/N 2x (w*x - y)\n",
    "def gradient(x,y,y_predicted):\n",
    "    return np.dot(2*x, y_predicted-y).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward(x)\n",
    "    # loss\n",
    "    l = loss(y, y_pred)\n",
    "    # gradient\n",
    "    dw = gradient(x,y,y_pred)\n",
    "    # update the weights\n",
    "    w -= learning_rate*dw\n",
    "    if epoch % 2 == 0:\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ],
   "id": "9229fe8d3e756ec3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 1.200, loss = 30.00000000\n",
      "epoch 3: w = 1.872, loss = 0.76800019\n",
      "epoch 5: w = 1.980, loss = 0.01966083\n",
      "epoch 7: w = 1.997, loss = 0.00050332\n",
      "epoch 9: w = 1.999, loss = 0.00001288\n",
      "epoch 11: w = 2.000, loss = 0.00000033\n",
      "epoch 13: w = 2.000, loss = 0.00000001\n",
      "epoch 15: w = 2.000, loss = 0.00000000\n",
      "epoch 17: w = 2.000, loss = 0.00000000\n",
      "epoch 19: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T21:40:17.965292Z",
     "start_time": "2025-02-04T21:40:17.951967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#gradient calculation with torch\n",
    "import torch\n",
    "\n",
    "# f = w * x\n",
    "# f = 2 * x\n",
    "x = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# calculate the model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "# calculate the loss = MSE\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted - y) ** 2).mean()\n",
    "# calculate the gradient\n",
    "# MSE = 1/N * (w*x - y)**2\n",
    "# dJ/dw = 1/N 2x (w*x - y)\n",
    "def gradient(x,y,y_predicted):\n",
    "    return np.dot(2*x, y_predicted-y).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward(x)\n",
    "    # loss\n",
    "    l = loss(y, y_pred)\n",
    "    # gradient = backward pass\n",
    "    l.backward() #dl/dw\n",
    "    # update the weights\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate*w.grad\n",
    "    \n",
    "    # zero gradients\n",
    "    w.grad.zero_()\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ],
   "id": "8a7959a25fc9ad57",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 3: w = 0.772, loss = 15.66018772\n",
      "epoch 5: w = 1.113, loss = 8.17471695\n",
      "epoch 7: w = 1.359, loss = 4.26725292\n",
      "epoch 9: w = 1.537, loss = 2.22753215\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 13: w = 1.758, loss = 0.60698116\n",
      "epoch 15: w = 1.825, loss = 0.31684780\n",
      "epoch 17: w = 1.874, loss = 0.16539653\n",
      "epoch 19: w = 1.909, loss = 0.08633806\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "epoch 23: w = 1.952, loss = 0.02352631\n",
      "epoch 25: w = 1.966, loss = 0.01228084\n",
      "epoch 27: w = 1.975, loss = 0.00641066\n",
      "epoch 29: w = 1.982, loss = 0.00334642\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "epoch 33: w = 1.991, loss = 0.00091188\n",
      "epoch 35: w = 1.993, loss = 0.00047601\n",
      "epoch 37: w = 1.995, loss = 0.00024848\n",
      "epoch 39: w = 1.996, loss = 0.00012971\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 43: w = 1.998, loss = 0.00003534\n",
      "epoch 45: w = 1.999, loss = 0.00001845\n",
      "epoch 47: w = 1.999, loss = 0.00000963\n",
      "epoch 49: w = 1.999, loss = 0.00000503\n",
      "epoch 51: w = 1.999, loss = 0.00000262\n",
      "epoch 53: w = 2.000, loss = 0.00000137\n",
      "epoch 55: w = 2.000, loss = 0.00000071\n",
      "epoch 57: w = 2.000, loss = 0.00000037\n",
      "epoch 59: w = 2.000, loss = 0.00000019\n",
      "epoch 61: w = 2.000, loss = 0.00000010\n",
      "epoch 63: w = 2.000, loss = 0.00000005\n",
      "epoch 65: w = 2.000, loss = 0.00000003\n",
      "epoch 67: w = 2.000, loss = 0.00000001\n",
      "epoch 69: w = 2.000, loss = 0.00000001\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 73: w = 2.000, loss = 0.00000000\n",
      "epoch 75: w = 2.000, loss = 0.00000000\n",
      "epoch 77: w = 2.000, loss = 0.00000000\n",
      "epoch 79: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 83: w = 2.000, loss = 0.00000000\n",
      "epoch 85: w = 2.000, loss = 0.00000000\n",
      "epoch 87: w = 2.000, loss = 0.00000000\n",
      "epoch 89: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "epoch 93: w = 2.000, loss = 0.00000000\n",
      "epoch 95: w = 2.000, loss = 0.00000000\n",
      "epoch 97: w = 2.000, loss = 0.00000000\n",
      "epoch 99: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T22:26:16.835788Z",
     "start_time": "2025-02-04T22:26:15.424543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1) Design model (input, output size, forward pass)\n",
    "# 2) Construct the loss and optimizer\n",
    "# 3) Training loop \n",
    "#   - Forward pass: compute the prediction \n",
    "#   - Backward pass: compute the gradients\n",
    "#   - Update the weights \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# f = w * x\n",
    "# f = 2 * x\n",
    "x = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# calculate the model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w], lr=learning_rate)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward(x)\n",
    "    # loss\n",
    "    l = loss(y, y_pred)\n",
    "    # gradient = backward pass\n",
    "    l.backward() #dl/dw\n",
    "    # update the weights\n",
    "    optimizer.step()\n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "\n"
   ],
   "id": "191ff850a3228121",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 3: w = 0.772, loss = 15.66018772\n",
      "epoch 5: w = 1.113, loss = 8.17471695\n",
      "epoch 7: w = 1.359, loss = 4.26725292\n",
      "epoch 9: w = 1.537, loss = 2.22753215\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 13: w = 1.758, loss = 0.60698116\n",
      "epoch 15: w = 1.825, loss = 0.31684780\n",
      "epoch 17: w = 1.874, loss = 0.16539653\n",
      "epoch 19: w = 1.909, loss = 0.08633806\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "epoch 23: w = 1.952, loss = 0.02352631\n",
      "epoch 25: w = 1.966, loss = 0.01228084\n",
      "epoch 27: w = 1.975, loss = 0.00641066\n",
      "epoch 29: w = 1.982, loss = 0.00334642\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "epoch 33: w = 1.991, loss = 0.00091188\n",
      "epoch 35: w = 1.993, loss = 0.00047601\n",
      "epoch 37: w = 1.995, loss = 0.00024848\n",
      "epoch 39: w = 1.996, loss = 0.00012971\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 43: w = 1.998, loss = 0.00003534\n",
      "epoch 45: w = 1.999, loss = 0.00001845\n",
      "epoch 47: w = 1.999, loss = 0.00000963\n",
      "epoch 49: w = 1.999, loss = 0.00000503\n",
      "epoch 51: w = 1.999, loss = 0.00000262\n",
      "epoch 53: w = 2.000, loss = 0.00000137\n",
      "epoch 55: w = 2.000, loss = 0.00000071\n",
      "epoch 57: w = 2.000, loss = 0.00000037\n",
      "epoch 59: w = 2.000, loss = 0.00000019\n",
      "epoch 61: w = 2.000, loss = 0.00000010\n",
      "epoch 63: w = 2.000, loss = 0.00000005\n",
      "epoch 65: w = 2.000, loss = 0.00000003\n",
      "epoch 67: w = 2.000, loss = 0.00000001\n",
      "epoch 69: w = 2.000, loss = 0.00000001\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 73: w = 2.000, loss = 0.00000000\n",
      "epoch 75: w = 2.000, loss = 0.00000000\n",
      "epoch 77: w = 2.000, loss = 0.00000000\n",
      "epoch 79: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 83: w = 2.000, loss = 0.00000000\n",
      "epoch 85: w = 2.000, loss = 0.00000000\n",
      "epoch 87: w = 2.000, loss = 0.00000000\n",
      "epoch 89: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "epoch 93: w = 2.000, loss = 0.00000000\n",
      "epoch 95: w = 2.000, loss = 0.00000000\n",
      "epoch 97: w = 2.000, loss = 0.00000000\n",
      "epoch 99: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T22:35:57.858862Z",
     "start_time": "2025-02-04T22:35:57.838735Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "Prediction before training: f(5) = -2.809\n",
      "epoch 1: w = 0.018, loss = 54.00693512\n",
      "epoch 11: w = 1.700, loss = 1.39800382\n",
      "epoch 21: w = 1.970, loss = 0.03684373\n",
      "epoch 31: w = 2.013, loss = 0.00158810\n",
      "epoch 41: w = 2.019, loss = 0.00063898\n",
      "epoch 51: w = 2.020, loss = 0.00057962\n",
      "epoch 61: w = 2.019, loss = 0.00054531\n",
      "epoch 71: w = 2.019, loss = 0.00051355\n",
      "epoch 81: w = 2.018, loss = 0.00048367\n",
      "epoch 91: w = 2.018, loss = 0.00045551\n",
      "Prediction after training: f(5) = 10.036\n"
     ]
    }
   ],
   "execution_count": 21,
   "source": [
    "# 1) Design model (input, output size, forward pass)\n",
    "# 2) Construct the loss and optimizer\n",
    "# 3) Training loop \n",
    "#   - Forward pass: compute the prediction \n",
    "#   - Backward pass: compute the gradients\n",
    "#   - Update the weights \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# f = w * x\n",
    "# f = 2 * x\n",
    "x = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "\n",
    "x_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = x.shape\n",
    "print(n_samples, n_features)\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "# model = nn.Linear(input_size, output_size)\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        #define layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "    \n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "print(f'Prediction before training: f(5) = {model(x_test).item():.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = model(x)\n",
    "    # loss\n",
    "    l = loss(y, y_pred)\n",
    "    # gradient = backward pass\n",
    "    l.backward() #dl/dw\n",
    "    # update the weights\n",
    "    optimizer.step()\n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        [w, b] = model.parameters()\n",
    "        print(f'epoch {epoch + 1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {model(x_test).item():.3f}')\n",
    "\n",
    "\n"
   ],
   "id": "575255e06b7f0894"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "LINEAR REGRESSION",
   "id": "6b21f56f5b27dd8c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T22:33:48.929907Z",
     "start_time": "2025-02-05T22:33:46.666698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1) Design model (input, output size, forward pass)\n",
    "# 2) Construct loss and optimizer\n",
    "# 3) Training loop\n",
    "#  - forward pass: compute prediction and loss\n",
    "#  - backward pass: gradients\n",
    "#  - update weights\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 0) Prepare data\n",
    "x_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=1)\n",
    "x = torch.from_numpy(x_numpy.astype(np.float32))\n",
    "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
    "# reshape the tensor to a column vector\n",
    "y = y.view(y.shape[0], 1)\n",
    "\n",
    "n_samples, n_features = x.shape\n",
    "# 1) model design \n",
    "input_size = n_features\n",
    "output_size = 1\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# 2) define loss and optimizer\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# 3) training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # forward pass and loss\n",
    "    y_predicted = model(x)\n",
    "    loss = criterion(y_predicted, y)\n",
    "    # backward pass (backpropagation) i.e. calculation of the derivatives\n",
    "    loss.backward()\n",
    "    # update the weights\n",
    "    optimizer.step() # updates the weights. The w is already in the optimizer, so no need to call w.grad to calculate the new weights\n",
    "    optimizer.zero_grad() # empty out the gradients before the next iterations. Otherwise, the backward function will sum up the gradients into the .grad attribute(.grad is internal to the optimizer in this case)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'epoch: {epoch + 1}, loss: {loss.item():.4f}')\n",
    "        \n",
    "#plot\n",
    "predicted = model(x).detach() # the detach disables the tensor from being tracked on the computational graph\n",
    "plt.plot(x_numpy, y_numpy, 'ro')\n",
    "plt.plot(x_numpy, predicted, 'b')\n",
    "plt.show()"
   ],
   "id": "81b40f50f3dc1c47",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss: 4328.2764\n",
      "epoch: 20, loss: 3229.2681\n",
      "epoch: 30, loss: 2434.4775\n",
      "epoch: 40, loss: 1859.0608\n",
      "epoch: 50, loss: 1442.0402\n",
      "epoch: 60, loss: 1139.5271\n",
      "epoch: 70, loss: 919.8884\n",
      "epoch: 80, loss: 760.2917\n",
      "epoch: 90, loss: 644.2373\n",
      "epoch: 100, loss: 559.7878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "LOGISTIC REGRESSION ",
   "id": "13072263aedc1028"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1) Design model (input, output size, forward pass)\n",
    "# 2) Construct loss and optimizer\n",
    "# 3) Training loop\n",
    "#  - forward pass: compute prediction and loss\n",
    "#  - backward pass: gradients\n",
    "#  - update weights\n",
    "\n"
   ],
   "id": "22b9c660ec167f00"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c8ecf37ba894cf47"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8bfeef12ebd3753f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "676522a2bfefc50b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8ba79c21a8d659f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6e0b8e4c90a6eb26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4cda4d9e2dd894fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b17f2545eab07a46"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "13a9bae2c0c096f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "32ef86e04539d5e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bb2d9ff75b991974"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9cc93659be767dad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4caa9dd186446883"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a37a3cb6f3d29b13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f8279eaac9e8b34b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6ac714482e91849b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3886228dc79f4cd9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cd9d2e82546ffbee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a02ee922fa90d0b6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
