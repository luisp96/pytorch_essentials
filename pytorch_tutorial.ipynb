{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-04T02:57:22.430061Z",
     "start_time": "2025-02-04T02:57:20.995784Z"
    }
   },
   "source": [
    "import torch\n",
    "from numpy import dtype\n",
    "from sympy.codegen.ast import float32\n",
    "from torchvision.transforms.v2 import ToTensor\n",
    "\n",
    "x = torch.rand(3)\n",
    "print(x)\n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "x = torch.empty(3)\n",
    "print(x)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1361, 0.9594, 0.1303])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x = torch.empty(2, 3)\n",
    "print(x)"
   ],
   "id": "ca7aaaf5d1731352"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x = torch.empty(2, 3, 2)\n",
    "print(x)"
   ],
   "id": "25599daaede69323"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x = torch.empty(2, 3, 2, 2)\n",
    "print(x)"
   ],
   "id": "503090d1d04da03a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x = torch.rand(2,2)\n",
    "print(x)"
   ],
   "id": "30df6d240acb7a48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x = torch.zeros(2,2)\n",
    "print(x)"
   ],
   "id": "68d786a4fe2ff35d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x = torch.ones(2,2)\n",
    "print(x)"
   ],
   "id": "d5f3adff041544c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x = torch.ones(2,2)\n",
    "print(x.dtype)"
   ],
   "id": "f4e1a70e2582fdc9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x = torch.ones(2,2, dtype=torch.int) #torch.double, torch.float16, torch.float64\n",
    "print(x.dtype) #prints the dtype of the current tensor"
   ],
   "id": "f05cf91f7f4cd89b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x = torch.ones(2,2, dtype=torch.float16)\n",
    "print(x.size()) # prints the size of the current tensor"
   ],
   "id": "bde25427470147e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x = torch.tensor([2.5, 0.1])\n",
    "print(x)"
   ],
   "id": "fc07c30934b2dc32"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "BASIC OPERATIONS",
   "id": "2556f15d79d1c98f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x = torch.rand(2,2)\n",
    "y = torch.rand(2,2)\n",
    "print(x)\n",
    "print(y)\n",
    "z = x + y\n",
    "z = torch.add(x,y)\n",
    "print(z) # it does element wise addition\n"
   ],
   "id": "b9744c58621c7d24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "y.add_(x) # inplace operation. Add all of the x's to the y. It modifies the variable in which it is applied to\n",
    "print(y)"
   ],
   "id": "e4eb8fca20db4f9d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "z = x - y\n",
    "z = torch.sub(x,y)\n",
    "print(z)"
   ],
   "id": "da9ab11c3c3b5236"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "z = x * y\n",
    "z = torch.mul(x,y)\n",
    "print(z)\n",
    "y.mul_(x) #inplace operation"
   ],
   "id": "89abdf50080bf0f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "z = x / y\n",
    "z = torch.div(x,y)"
   ],
   "id": "51256b0dee34b3cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#slciing operations\n",
    "x = torch.rand(5,3)\n",
    "print(x[:,0]) #slicing for all rows but only the first column of each row\n",
    "print(x[1, :]) #slicing for only the first row but all of the columns from said row\n",
    "#you can also get the value of a tensor, if and only if is a single element tensor\n",
    "print(x[1, 1].item())\n"
   ],
   "id": "d8b65094524e5c0b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# reshaping a tensor\n",
    "x = torch.rand(4,4)\n",
    "print(x)\n",
    "y = x.view(16) #resizes the 16 values in a 4x4 tensor and a single 1x16. NOTE: the number of elements must match\n",
    "print(y)\n",
    "\n",
    "y = x.view(-1, 8) #if we don't want to resize to a single dimension tensor, then we can put a -1 on the first position and pytorch will correctly assume the dimension for the number of rows. i.e. 2x8\n",
    "print(y.size)"
   ],
   "id": "3b7a34c76b436bbb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:24:49.119830Z",
     "start_time": "2025-02-04T03:24:49.112657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#converting from numpy to tensor and viceversa\n",
    "import numpy as np\n",
    "# convert from a tensor into a np array\n",
    "a = torch.ones(5)\n",
    "print(a)\n",
    "b = a.numpy()\n",
    "print(type(b))  #i.e. numpy ndarray\n",
    "\n",
    "#if the tensor is in the cpu, then both objects(tensor and nparray) will share the same memory location, so any change to either will reflect on the other\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ],
   "id": "8d07a966d0a43a04",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "<class 'numpy.ndarray'>\n",
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:26:30.273950Z",
     "start_time": "2025-02-04T03:26:30.267841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#convert from a numpy array to a tensor\n",
    "a = np.ones(5)\n",
    "print(a)\n",
    "b = torch.from_numpy(a) #be careful changing the dtype, both must match. Otherwise you get an error\n",
    "print(b)\n",
    "\n",
    "a += 1\n",
    "print(a)\n",
    "print(b)\n"
   ],
   "id": "2ac57e2f9e1e065b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:31:13.450574Z",
     "start_time": "2025-02-04T03:31:13.436679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#gpu operations\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    x = torch.ones(5, device=device) #creates a tensor in the GPU \n",
    "    y = torch.ones(5) \n",
    "    y = y.to(device) # this moves the tensor to the GPU\n",
    "    z = x + y # this will be computed in the GPU\n",
    "    # you can't convert a GPU tensor into a numpy ndarray, so you must first move it into the cpu(numpy can only handle cpu tensors)\n",
    "    z = z.to(\"cpu\")\n",
    "    print(z)"
   ],
   "id": "b6fb73da74848bf7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "x = torch.ones(5, requires_grad=True) #the requires_grad flag is FALSE by default. It tells pytorch that it will need to calculate the gradients for this tensor later in the optimization steps. \n",
    "print(x) # this will also print the requires_grad flag as well"
   ],
   "id": "5f0ff1aa6e5b7e86"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Gradient Calculations with autograd",
   "id": "d9e7f31a380fc5d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:38:54.574281Z",
     "start_time": "2025-02-04T03:38:54.567676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#gradients are essential for optimization\n",
    "import torch \n",
    "\n",
    "x = torch.rand(3, requires_grad=True) # must specify the requires_grad to calculate the gradients\n",
    "print(x)\n",
    "y = x + 2 # pytorch creates a computational graph\n",
    "# pytorch will compute the forward pass, and create a grad_fn function and calculate the gradients during the backpropagation step(dy/dx)\n",
    "print(y) #has a grad_fn for AddBackward\n",
    "z = y*y*2\n",
    "print(z) #has a grad_fn for MulBackward\n",
    "z = z.mean()\n",
    "print(z) # has a grad_fn for MeanBackward\n",
    "\n",
    "# now when we calculate the gradients for z \n",
    "z.backward() #backward gradient calculation  dz/dx. The backward function will only work with scalar values(single value), if called without an argument\n",
    "print(x.grad) # the gradients are stored in x.grad\n",
    "#NOTE if we don't specify the requires_grad, then we won't have a .backward() grad_fn, giving us an error\n",
    " \n",
    "\n"
   ],
   "id": "91dbf909bca1c55b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5928, 0.9772, 0.1487], requires_grad=True)\n",
      "tensor([2.5928, 2.9772, 2.1487], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The gradient calculation calculates a Jacobian product to get the gradients. We multiply the jacobian matrix containing the partial derivatives with a gradient vector(of the same size )",
   "id": "1494434079cb57b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.001], dtype=torch.float32)\n",
    "z.backward(v) #dz/dx\n",
    "print(x.grad)"
   ],
   "id": "270eecacae975a00"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:59:33.534715Z",
     "start_time": "2025-02-04T03:59:33.529262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# sometimes during our training, when we want update the weight, we don't want pytorch to calculate the gradients\n",
    "# we have 3 options for this\n",
    "\n",
    "# x.requires_grad_(False)\n",
    "# x.detach()\n",
    "# with torch.no_grad()\n",
    "\n",
    "x.requires_grad_(False) # the underscore means that pytorch will modify the variable in place\n",
    "print(x)\n",
    "\n",
    "y = x.detach() # creates a new tensor with the same values but it doesn't requrie the gradients (so requires_grad = False)\n",
    "print(y)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = x + 2\n",
    "    print(z)"
   ],
   "id": "693ea07e74630dff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5928, 0.9772, 0.1487])\n",
      "tensor([0.5928, 0.9772, 0.1487])\n",
      "tensor([2.5928, 2.9772, 2.1487])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T04:03:32.997796Z",
     "start_time": "2025-02-04T04:03:32.993221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# VERY IMPORTANT: Whenever we call the backward function, then the gradient for the tensor will be accumulated into the .grad attribute, so the values will be summed up\n",
    "weights = torch.ones(4, requires_grad=True)\n",
    "for epoch in range(3):\n",
    "    model_output = (weights*3).sum()\n",
    "    \n",
    "    model_output.backward()\n",
    "    \n",
    "    print(weights.grad) #gradients get aggregated [[3...],[6....],[9.....]]\n",
    "    weights.grad.zero_() #you must empty the gradients after each iteration to get the correct gradients [3....]\n",
    "    \n",
    "# same thing but with an optimizer\n",
    "optimizer = torch.optim.SGD(weights, lr=0.01)\n",
    "optimizer.step() # next iteration\n",
    "optimizer.zero_grad() # empty the gradients "
   ],
   "id": "226ece1ae8d3919e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "BACKPROPAGATION - THEORY WITH EXAMPLES",
   "id": "1ea655f22661c3c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T04:21:10.021879Z",
     "start_time": "2025-02-04T04:21:10.013762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "#forward pass and compute the loss\n",
    "y_hat = w * x\n",
    "loss = (y_hat - y)**2\n",
    "print(loss)\n",
    "\n",
    "#backward pass\n",
    "loss.backward() #calculates the gradients during the backward pass and stores them in the tensors that have requires_grad \n",
    "print(w.grad) #prints the stored gradients for w\n",
    "\n",
    "##update weights\n",
    "## next forward and backward pass"
   ],
   "id": "bd08716d50bb05f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n",
      "tensor(-2.)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "GRADIENT DESCENT WITH AUTOGRAD AND BACKPROPAGATION",
   "id": "7917e2fd855354f3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T21:35:04.971891Z",
     "start_time": "2025-02-04T21:35:04.965750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# f = w * x\n",
    "# f = 2 * x\n",
    "x = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
    "\n",
    "w = 0.0\n",
    "\n",
    "# calculate the model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "# calculate the loss = MSE\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted - y) ** 2).mean()\n",
    "# calculate the gradient\n",
    "# MSE = 1/N * (w*x - y)**2\n",
    "# dJ/dw = 1/N 2x (w*x - y)\n",
    "def gradient(x,y,y_predicted):\n",
    "    return np.dot(2*x, y_predicted-y).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward(x)\n",
    "    # loss\n",
    "    l = loss(y, y_pred)\n",
    "    # gradient\n",
    "    dw = gradient(x,y,y_pred)\n",
    "    # update the weights\n",
    "    w -= learning_rate*dw\n",
    "    if epoch % 2 == 0:\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ],
   "id": "9229fe8d3e756ec3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 1.200, loss = 30.00000000\n",
      "epoch 3: w = 1.872, loss = 0.76800019\n",
      "epoch 5: w = 1.980, loss = 0.01966083\n",
      "epoch 7: w = 1.997, loss = 0.00050332\n",
      "epoch 9: w = 1.999, loss = 0.00001288\n",
      "epoch 11: w = 2.000, loss = 0.00000033\n",
      "epoch 13: w = 2.000, loss = 0.00000001\n",
      "epoch 15: w = 2.000, loss = 0.00000000\n",
      "epoch 17: w = 2.000, loss = 0.00000000\n",
      "epoch 19: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T21:40:17.965292Z",
     "start_time": "2025-02-04T21:40:17.951967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#gradient calculation with torch\n",
    "import torch\n",
    "\n",
    "# f = w * x\n",
    "# f = 2 * x\n",
    "x = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# calculate the model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "# calculate the loss = MSE\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted - y) ** 2).mean()\n",
    "# calculate the gradient\n",
    "# MSE = 1/N * (w*x - y)**2\n",
    "# dJ/dw = 1/N 2x (w*x - y)\n",
    "def gradient(x,y,y_predicted):\n",
    "    return np.dot(2*x, y_predicted-y).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward(x)\n",
    "    # loss\n",
    "    l = loss(y, y_pred)\n",
    "    # gradient = backward pass\n",
    "    l.backward() #dl/dw\n",
    "    # update the weights\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate*w.grad\n",
    "    \n",
    "    # zero gradients\n",
    "    w.grad.zero_()\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ],
   "id": "8a7959a25fc9ad57",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 3: w = 0.772, loss = 15.66018772\n",
      "epoch 5: w = 1.113, loss = 8.17471695\n",
      "epoch 7: w = 1.359, loss = 4.26725292\n",
      "epoch 9: w = 1.537, loss = 2.22753215\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 13: w = 1.758, loss = 0.60698116\n",
      "epoch 15: w = 1.825, loss = 0.31684780\n",
      "epoch 17: w = 1.874, loss = 0.16539653\n",
      "epoch 19: w = 1.909, loss = 0.08633806\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "epoch 23: w = 1.952, loss = 0.02352631\n",
      "epoch 25: w = 1.966, loss = 0.01228084\n",
      "epoch 27: w = 1.975, loss = 0.00641066\n",
      "epoch 29: w = 1.982, loss = 0.00334642\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "epoch 33: w = 1.991, loss = 0.00091188\n",
      "epoch 35: w = 1.993, loss = 0.00047601\n",
      "epoch 37: w = 1.995, loss = 0.00024848\n",
      "epoch 39: w = 1.996, loss = 0.00012971\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 43: w = 1.998, loss = 0.00003534\n",
      "epoch 45: w = 1.999, loss = 0.00001845\n",
      "epoch 47: w = 1.999, loss = 0.00000963\n",
      "epoch 49: w = 1.999, loss = 0.00000503\n",
      "epoch 51: w = 1.999, loss = 0.00000262\n",
      "epoch 53: w = 2.000, loss = 0.00000137\n",
      "epoch 55: w = 2.000, loss = 0.00000071\n",
      "epoch 57: w = 2.000, loss = 0.00000037\n",
      "epoch 59: w = 2.000, loss = 0.00000019\n",
      "epoch 61: w = 2.000, loss = 0.00000010\n",
      "epoch 63: w = 2.000, loss = 0.00000005\n",
      "epoch 65: w = 2.000, loss = 0.00000003\n",
      "epoch 67: w = 2.000, loss = 0.00000001\n",
      "epoch 69: w = 2.000, loss = 0.00000001\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 73: w = 2.000, loss = 0.00000000\n",
      "epoch 75: w = 2.000, loss = 0.00000000\n",
      "epoch 77: w = 2.000, loss = 0.00000000\n",
      "epoch 79: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 83: w = 2.000, loss = 0.00000000\n",
      "epoch 85: w = 2.000, loss = 0.00000000\n",
      "epoch 87: w = 2.000, loss = 0.00000000\n",
      "epoch 89: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "epoch 93: w = 2.000, loss = 0.00000000\n",
      "epoch 95: w = 2.000, loss = 0.00000000\n",
      "epoch 97: w = 2.000, loss = 0.00000000\n",
      "epoch 99: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T22:26:16.835788Z",
     "start_time": "2025-02-04T22:26:15.424543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1) Design model (input, output size, forward pass)\n",
    "# 2) Construct the loss and optimizer\n",
    "# 3) Training loop \n",
    "#   - Forward pass: compute the prediction \n",
    "#   - Backward pass: compute the gradients\n",
    "#   - Update the weights \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# f = w * x\n",
    "# f = 2 * x\n",
    "x = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# calculate the model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w], lr=learning_rate)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward(x)\n",
    "    # loss\n",
    "    l = loss(y, y_pred)\n",
    "    # gradient = backward pass\n",
    "    l.backward() #dl/dw\n",
    "    # update the weights\n",
    "    optimizer.step()\n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "\n"
   ],
   "id": "191ff850a3228121",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 3: w = 0.772, loss = 15.66018772\n",
      "epoch 5: w = 1.113, loss = 8.17471695\n",
      "epoch 7: w = 1.359, loss = 4.26725292\n",
      "epoch 9: w = 1.537, loss = 2.22753215\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 13: w = 1.758, loss = 0.60698116\n",
      "epoch 15: w = 1.825, loss = 0.31684780\n",
      "epoch 17: w = 1.874, loss = 0.16539653\n",
      "epoch 19: w = 1.909, loss = 0.08633806\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "epoch 23: w = 1.952, loss = 0.02352631\n",
      "epoch 25: w = 1.966, loss = 0.01228084\n",
      "epoch 27: w = 1.975, loss = 0.00641066\n",
      "epoch 29: w = 1.982, loss = 0.00334642\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "epoch 33: w = 1.991, loss = 0.00091188\n",
      "epoch 35: w = 1.993, loss = 0.00047601\n",
      "epoch 37: w = 1.995, loss = 0.00024848\n",
      "epoch 39: w = 1.996, loss = 0.00012971\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 43: w = 1.998, loss = 0.00003534\n",
      "epoch 45: w = 1.999, loss = 0.00001845\n",
      "epoch 47: w = 1.999, loss = 0.00000963\n",
      "epoch 49: w = 1.999, loss = 0.00000503\n",
      "epoch 51: w = 1.999, loss = 0.00000262\n",
      "epoch 53: w = 2.000, loss = 0.00000137\n",
      "epoch 55: w = 2.000, loss = 0.00000071\n",
      "epoch 57: w = 2.000, loss = 0.00000037\n",
      "epoch 59: w = 2.000, loss = 0.00000019\n",
      "epoch 61: w = 2.000, loss = 0.00000010\n",
      "epoch 63: w = 2.000, loss = 0.00000005\n",
      "epoch 65: w = 2.000, loss = 0.00000003\n",
      "epoch 67: w = 2.000, loss = 0.00000001\n",
      "epoch 69: w = 2.000, loss = 0.00000001\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 73: w = 2.000, loss = 0.00000000\n",
      "epoch 75: w = 2.000, loss = 0.00000000\n",
      "epoch 77: w = 2.000, loss = 0.00000000\n",
      "epoch 79: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 83: w = 2.000, loss = 0.00000000\n",
      "epoch 85: w = 2.000, loss = 0.00000000\n",
      "epoch 87: w = 2.000, loss = 0.00000000\n",
      "epoch 89: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "epoch 93: w = 2.000, loss = 0.00000000\n",
      "epoch 95: w = 2.000, loss = 0.00000000\n",
      "epoch 97: w = 2.000, loss = 0.00000000\n",
      "epoch 99: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T22:35:57.858862Z",
     "start_time": "2025-02-04T22:35:57.838735Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "Prediction before training: f(5) = -2.809\n",
      "epoch 1: w = 0.018, loss = 54.00693512\n",
      "epoch 11: w = 1.700, loss = 1.39800382\n",
      "epoch 21: w = 1.970, loss = 0.03684373\n",
      "epoch 31: w = 2.013, loss = 0.00158810\n",
      "epoch 41: w = 2.019, loss = 0.00063898\n",
      "epoch 51: w = 2.020, loss = 0.00057962\n",
      "epoch 61: w = 2.019, loss = 0.00054531\n",
      "epoch 71: w = 2.019, loss = 0.00051355\n",
      "epoch 81: w = 2.018, loss = 0.00048367\n",
      "epoch 91: w = 2.018, loss = 0.00045551\n",
      "Prediction after training: f(5) = 10.036\n"
     ]
    }
   ],
   "execution_count": 21,
   "source": [
    "# 1) Design model (input, output size, forward pass)\n",
    "# 2) Construct the loss and optimizer\n",
    "# 3) Training loop \n",
    "#   - Forward pass: compute the prediction \n",
    "#   - Backward pass: compute the gradients\n",
    "#   - Update the weights \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# f = w * x\n",
    "# f = 2 * x\n",
    "x = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "\n",
    "x_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = x.shape\n",
    "print(n_samples, n_features)\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "# model = nn.Linear(input_size, output_size)\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        #define layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "    \n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "print(f'Prediction before training: f(5) = {model(x_test).item():.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = model(x)\n",
    "    # loss\n",
    "    l = loss(y, y_pred)\n",
    "    # gradient = backward pass\n",
    "    l.backward() #dl/dw\n",
    "    # update the weights\n",
    "    optimizer.step()\n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        [w, b] = model.parameters()\n",
    "        print(f'epoch {epoch + 1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {model(x_test).item():.3f}')\n",
    "\n",
    "\n"
   ],
   "id": "575255e06b7f0894"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "LINEAR REGRESSION",
   "id": "6b21f56f5b27dd8c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T22:33:48.929907Z",
     "start_time": "2025-02-05T22:33:46.666698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1) Design model (input, output size, forward pass)\n",
    "# 2) Construct loss and optimizer\n",
    "# 3) Training loop\n",
    "#  - forward pass: compute prediction and loss\n",
    "#  - backward pass: gradients\n",
    "#  - update weights\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 0) Prepare data\n",
    "x_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=1)\n",
    "x = torch.from_numpy(x_numpy.astype(np.float32))\n",
    "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
    "# reshape the tensor to a column vector\n",
    "y = y.view(y.shape[0], 1)\n",
    "\n",
    "n_samples, n_features = x.shape\n",
    "# 1) model design \n",
    "input_size = n_features\n",
    "output_size = 1\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# 2) define loss and optimizer\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# 3) training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # forward pass and loss\n",
    "    y_predicted = model(x)\n",
    "    loss = criterion(y_predicted, y)\n",
    "    # backward pass (backpropagation) i.e. calculation of the derivatives\n",
    "    loss.backward()\n",
    "    # update the weights\n",
    "    optimizer.step() # updates the weights. The w is already in the optimizer, so no need to call w.grad to calculate the new weights\n",
    "    optimizer.zero_grad() # empty out the gradients before the next iterations. Otherwise, the backward function will sum up the gradients into the .grad attribute(.grad is internal to the optimizer in this case)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'epoch: {epoch + 1}, loss: {loss.item():.4f}')\n",
    "        \n",
    "#plot\n",
    "predicted = model(x).detach() # the detach disables the tensor from being tracked on the computational graph\n",
    "plt.plot(x_numpy, y_numpy, 'ro')\n",
    "plt.plot(x_numpy, predicted, 'b')\n",
    "plt.show()"
   ],
   "id": "81b40f50f3dc1c47",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss: 4328.2764\n",
      "epoch: 20, loss: 3229.2681\n",
      "epoch: 30, loss: 2434.4775\n",
      "epoch: 40, loss: 1859.0608\n",
      "epoch: 50, loss: 1442.0402\n",
      "epoch: 60, loss: 1139.5271\n",
      "epoch: 70, loss: 919.8884\n",
      "epoch: 80, loss: 760.2917\n",
      "epoch: 90, loss: 644.2373\n",
      "epoch: 100, loss: 559.7878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "LOGISTIC REGRESSION ",
   "id": "13072263aedc1028"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T23:25:47.576782Z",
     "start_time": "2025-02-05T23:25:47.525757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1) Design model (input, output size, forward pass)\n",
    "# 2) Construct loss and optimizer\n",
    "# 3) Training loop\n",
    "#  - forward pass: compute prediction and loss\n",
    "#  - backward pass: gradients\n",
    "#  - update weights\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 0) prepare data\n",
    "bc = datasets.load_breast_cancer()\n",
    "x, y = bc.data, bc.target\n",
    "\n",
    "n_samples, n_features = x.shape\n",
    "print(n_samples, n_features)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1234)\n",
    "# scale the features\n",
    "sc = StandardScaler() #when dealing with logistic, we want to scale our data to have zero mean and uniform variance \n",
    "X_train = sc.fit_transform(x_train) #what is the difference between fit_transform and just transform?\n",
    "X_test = sc.transform(x_test)\n",
    "\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32)) # make sure to cast as np.float32 because the original data is of type Double and will cause problems down the road\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "y_train = y_train.view(y_train.shape[0], 1) # what is this shape change?\n",
    "y_test = y_test.view(y_test.shape[0], 1)\n",
    "\n",
    "# 1) setup the model \n",
    "# f = wx + b, sigmoid function at the end \n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, n_input_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y_predicted = torch.sigmoid(self.linear(x))\n",
    "        return y_predicted\n",
    "    \n",
    "model = LogisticRegression(n_features)\n",
    "\n",
    "# 2) setup the loss and optimizer\n",
    "learning_rate = 0.01\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3) training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    #forward pass and loss calculation\n",
    "    y_predicted = model(X_train)\n",
    "    loss = criterion(y_predicted, y_train)\n",
    "    #backward pass\n",
    "    loss.backward()\n",
    "    #updates weights\n",
    "    optimizer.step()\n",
    "    #zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'epoch: {epoch + 1}, loss: {loss.item():.4f}')\n",
    "        \n",
    "with torch.no_grad(): # for evaluation, we don't want the gradient calculations to be tracked in the computational graph for (model), so we must use torch.no_grad()\n",
    "    y_predicted = model(X_test)\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'Accuracy: {acc:.4f}')"
   ],
   "id": "22b9c660ec167f00",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569 30\n",
      "epoch: 10, loss: 0.4224\n",
      "epoch: 20, loss: 0.3759\n",
      "epoch: 30, loss: 0.3416\n",
      "epoch: 40, loss: 0.3151\n",
      "epoch: 50, loss: 0.2938\n",
      "epoch: 60, loss: 0.2764\n",
      "epoch: 70, loss: 0.2617\n",
      "epoch: 80, loss: 0.2491\n",
      "epoch: 90, loss: 0.2383\n",
      "epoch: 100, loss: 0.2287\n",
      "Accuracy: 0.8947\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "DATASET AND DATALOADER  ",
   "id": "f861a6800f088e89"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T03:05:31.194012Z",
     "start_time": "2025-02-06T03:05:31.184417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "epoch = 1 forward and backward pass of ALL training samples\n",
    "batch_size = number of training samples in one forward & backward pass\n",
    "number of iterations = number of passes, each pass using [batch_size] number of samples\n",
    "e.g. 100 samples, batch_size=20 --> 100/20 = 5 iterations for 1 epoch\n",
    "'''\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class WineDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # data loading\n",
    "        xy = np.loadtxt('./wine.csv', delimiter=',', dtype=np.float32, skiprows=1)\n",
    "        self.x = torch.from_numpy(xy[:, 1:])\n",
    "        self.y = torch.from_numpy(xy[:, [0]]) # n_samples, 1\n",
    "        self.n_samples = xy.shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        # dataset[0] helps for indexing\n",
    "        return self.x[item], self.y[item]\n",
    "    def __len__(self):\n",
    "        # len(dataset)\n",
    "        return self.n_samples\n",
    "    \n",
    "dataset = WineDataset()\n",
    "# data iteration without dataloader\n",
    "# first_data = dataset[0]\n",
    "# features, labels = first_data\n",
    "# print(features, labels)\n",
    "\n",
    "# data iteration with dataloader\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=4, shuffle=True)\n",
    "# dataiter = iter(dataloader)\n",
    "# data = next(dataiter)\n",
    "# features, labels = data\n",
    "# print(features, labels)\n",
    "\n",
    "#training loop\n",
    "num_epochs = 2\n",
    "batch_size = 4\n",
    "total_samples = len(dataset)\n",
    "n_iterations = math.ceil(total_samples/batch_size)\n",
    "print(total_samples, n_iterations)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        # forward and backward pass, update the weights\n",
    "        if (i+5) % 5 ==0:\n",
    "            print(f'epoch: {epoch + 1}/{num_epochs}, step {i+1}/{n_iterations}, inputs {inputs.shape}')\n"
   ],
   "id": "c8ecf37ba894cf47",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178 45\n",
      "epoch: 1/2, step 1/45, inputs torch.Size([4, 13])\n",
      "epoch: 1/2, step 6/45, inputs torch.Size([4, 13])\n",
      "epoch: 1/2, step 11/45, inputs torch.Size([4, 13])\n",
      "epoch: 1/2, step 16/45, inputs torch.Size([4, 13])\n",
      "epoch: 1/2, step 21/45, inputs torch.Size([4, 13])\n",
      "epoch: 1/2, step 26/45, inputs torch.Size([4, 13])\n",
      "epoch: 1/2, step 31/45, inputs torch.Size([4, 13])\n",
      "epoch: 1/2, step 36/45, inputs torch.Size([4, 13])\n",
      "epoch: 1/2, step 41/45, inputs torch.Size([4, 13])\n",
      "epoch: 2/2, step 1/45, inputs torch.Size([4, 13])\n",
      "epoch: 2/2, step 6/45, inputs torch.Size([4, 13])\n",
      "epoch: 2/2, step 11/45, inputs torch.Size([4, 13])\n",
      "epoch: 2/2, step 16/45, inputs torch.Size([4, 13])\n",
      "epoch: 2/2, step 21/45, inputs torch.Size([4, 13])\n",
      "epoch: 2/2, step 26/45, inputs torch.Size([4, 13])\n",
      "epoch: 2/2, step 31/45, inputs torch.Size([4, 13])\n",
      "epoch: 2/2, step 36/45, inputs torch.Size([4, 13])\n",
      "epoch: 2/2, step 41/45, inputs torch.Size([4, 13])\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "DATASET TRANSFORM",
   "id": "9c4f78e242523917"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T08:04:28.994858Z",
     "start_time": "2025-02-11T08:04:28.981574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "Transforms can be applied to PIL images, tensors, ndarrays, or custom data during creation of the DataSet\n",
    "\n",
    "complete list of built-in transforms:\n",
    "https://pytorch.org/docs/stable/transforms.html\n",
    "\n",
    "On Images\n",
    "----------\n",
    "CenterCrop, Grayscale, Pad, RandomAffine\n",
    "RandomCrop, RandomHorizontalFlip, RandomVerticalFlip, \n",
    "RandomRotation, Resize, Scale\n",
    "\n",
    "On Tensors\n",
    "-----------\n",
    "LinearTransformation, Normalize, RandomErasing\n",
    "\n",
    "Conversion\n",
    "-----------\n",
    "ToPilImage: from tensor or ndarray\n",
    "ToTensor: from numpy.ndarray or PILImage\n",
    "\n",
    "Generic\n",
    "-----------\n",
    "Use Lambda\n",
    "\n",
    "Custom\n",
    "-----------\n",
    "Write own class\n",
    "\n",
    "Compose Multiple Transforms\n",
    "-----------------------------\n",
    "composed = transforms.Compose([Rescale(256),\n",
    "                                RandomCrop(224),])\n",
    "torchvision.transforms.ReScale(256)\n",
    "torchvision.transforms.ToTensor()\n",
    "\n",
    "dataset = torchvision.datasets.MNIST( root=\"./\", transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class WineDataset(Dataset):\n",
    "    def __init__(self, transform=None): # the = on the parameter indicates default value, which in this case would be None\n",
    "        # data loading\n",
    "        xy = np.loadtxt('./wine.csv', delimiter=',', dtype=np.float32, skiprows=1)\n",
    "        self.n_samples = xy.shape[0]\n",
    "        \n",
    "        #note that we do not convert to tensor here\n",
    "        self.x = xy[:, 1:]\n",
    "        self.y = xy[:, [0]]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        # dataset[0] helps for indexing\n",
    "        sample =  self.x[item], self.y[item]\n",
    "        # if there is a transform\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample) # applies the custom transform class. \n",
    "            \n",
    "        return sample  # return the sample, with a transform if one was specified\n",
    "            \n",
    "    def __len__(self):\n",
    "        # len(dataset)\n",
    "        return self.n_samples\n",
    "\n",
    "# custom transform class\n",
    "class ToTensor:\n",
    "    def __call__(self, sample): # the call function gets called automatically when we pass a sample into the ToTensor class as a function\n",
    "        inputs, targets = sample\n",
    "        return torch.from_numpy(inputs), torch.from_numpy(targets)\n",
    "    \n",
    "# a multiplication transform class\n",
    "class MulTransform:\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "    def __call__(self, sample):\n",
    "        inputs, targets = sample\n",
    "        inputs *= self.factor\n",
    "        return inputs, targets\n",
    "    \n",
    "dataset = WineDataset(transform=ToTensor()) \n",
    "first_data = dataset[0]\n",
    "features, labels = first_data\n",
    "print(features)\n",
    "print(type(features), type(labels)) # should be a torch.Tensor class\n",
    "\n",
    "composed = torchvision.transforms.Compose([ToTensor(), MulTransform(4)])\n",
    "dataset = WineDataset(transform=composed)\n",
    "first_data = dataset[0]\n",
    "features, labels = first_data\n",
    "print(features)\n",
    "print(type(features), type(labels)) # should be a torch.Tensor class\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "f5e6c143dd6e3110",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n",
      "        3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n",
      "        1.0650e+03])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "tensor([5.6920e+01, 6.8400e+00, 9.7200e+00, 6.2400e+01, 5.0800e+02, 1.1200e+01,\n",
      "        1.2240e+01, 1.1200e+00, 9.1600e+00, 2.2560e+01, 4.1600e+00, 1.5680e+01,\n",
      "        4.2600e+03])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "SOFTMAX AND CROSS-ENTROPY FUNCTIONS   ",
   "id": "e218e1522de60a35"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T08:28:42.428593Z",
     "start_time": "2025-02-11T08:28:42.417994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "The Softmax function squashes the raw outputs into probabilities i.e.[0 - 1]. It does this by applying the exponential function to each input\n",
    "and normalizing it by the sum of all inputs\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x), axis=0) # axis of x sums up the inputs across the columns, and y sums up the inputs across the rows.\n",
    "\n",
    "x = np.array([2.0,1.0, 0.1])\n",
    "outputs = softmax(x)\n",
    "print('softmax numpy:', outputs)\n",
    "\n",
    "# torch version of softmax\n",
    "x = torch.tensor([2.0, 1.0, 0.1])\n",
    "outputs = torch.softmax(x, dim=0) # dim is the same as axis for numpy\n",
    "print('softmax torch:', outputs)\n",
    "\n"
   ],
   "id": "7c32840319147ce4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax numpy: [0.65900114 0.24243297 0.09856589]\n",
      "softmax torch: tensor([0.6590, 0.2424, 0.0986])\n",
      "Loss1 numpy: 0.3567\n",
      "Loss2 numpy: 2.3026\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "'''\n",
    "A lot times the softmax is combined with the cross-entropy loss function. This measures the performance of our classification model, whose output is a probability between \n",
    "0 and 1. It can be used in multi class problems. The loss increases as the predictor probability diverges from the actual label. So the better the prediction, the lower the loss. \n",
    "Note: in the case for probability label prediction, our label (Y), needs to be one-hot encoded i.e. Y = [1,0,0]. \n",
    "\n",
    "Y = [1, 0, 0] (Actual class label)\n",
    "(predicted class label from softmax)\n",
    "Y_hat = [0.7, 0.2, 0.1]  \n",
    "Cross_entropy = D(Y, Y_hat) = 0.35 (GOOD PREDICTION)\n",
    "\n",
    "Y = [1, 0, 0] (Actual class label)\n",
    "(predicted class label from softmax)\n",
    "Y_hat = [0.1, 0.3, 0.6]  \n",
    "Cross_entropy = D(Y, Y_hat) = 2.30 (GOOD PREDICTION)\n",
    "'''\n",
    "\n",
    "# Cross-Entropy code numpy\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def cross_entropy(actual, predicted):\n",
    "    loss = -np.sum(actual * np.log(predicted))\n",
    "    return loss # / float(predicted.shape[0]) The cross-entropy eqn dictates we normalize the output \n",
    "\n",
    "# y must be one hot encoded \n",
    "# if class 0: [1 0 0]\n",
    "# if class 1: [0 1 0]\n",
    "# if class 2: [0 0 1]\n",
    "Y = np.array([1, 0, 0])\n",
    "\n",
    "# y_pred has probabilities\n",
    "Y_pred_good = np.array([0.7,0.2,0.1])\n",
    "Y_pred_bad = np.array([0.1, 0.3, 0.6])\n",
    "l1 = cross_entropy(Y, Y_pred_good)\n",
    "l2 = cross_entropy(Y, Y_pred_bad)\n",
    "print(f'Loss1 numpy: {l1:.4f}')\n",
    "print(f'Loss2 numpy: {l2:.4f}')"
   ],
   "id": "fc99870bb1dfebc4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T08:43:34.372422Z",
     "start_time": "2025-02-11T08:43:34.358972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "'''\n",
    "Careful! \n",
    "nn.CrossEntropyLoss() applies:\n",
    "nn.LogSoftmax() + nn.NLLLoss() (negative log likelihood loss)\n",
    "\n",
    "--> No Softmax in last layer! i.e. we should not implement the softmax ourselves, since it is already applied\n",
    "\n",
    "Y has clas labels, NOT ONE-HOT!\n",
    "Y_pred has raw scores(logits), NO SOFTMAX!\n",
    "'''\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "Y = torch.tensor([0]) # remember it is not one hot encoded, but rather class label\n",
    "# n_samples * n_classes = 1x3\n",
    "Y_pred_good = torch.tensor([[2.0, 1.0, 0.1]]) # the class prediction has the raw values, no softmax i.e. normalization\n",
    "Y_pred_bad = torch.tensor([[0.5, 2.0, 0.3]])\n",
    "\n",
    "l1 = loss(Y_pred_good, Y) # here order matters, loss(prediction, actual)\n",
    "l2 = loss(Y_pred_bad, Y)\n",
    "\n",
    "print(f'L1 loss tensor {l1.item()}')\n",
    "print(f'L2 loss tensor {l2.item()}')\n",
    "\n",
    "# actual predictions\n",
    "_, predictions1 = torch.max(Y_pred_good, dim=1)\n",
    "_, predictions2 = torch.max(Y_pred_bad, dim=1)\n",
    "print(predictions1)\n",
    "print(predictions2)\n",
    "\n",
    "# loss function with multiple samples\n",
    "# 3 samples\n",
    "Y = torch.tensor([2, 0, 1])\n",
    "# n_samples * n_classes = 3x3\n",
    "Y_pred_good = torch.tensor([[0.1, 1.0, 2.1], [2.0, 1.0, 0.1], [0.1, 3.0, 0.1]]) # the class prediction has the raw values, no softmax i.e. normalization\n",
    "Y_pred_bad = torch.tensor([[2.1, 1.0, 0.1], [0.1, 1.0, 2.1], [0.1, 3.0, 0.1]])\n",
    "\n",
    "l1 = loss(Y_pred_good, Y) # here order matters, loss(prediction, actual)\n",
    "l2 = loss(Y_pred_bad, Y)\n",
    "\n",
    "print(f'L1 loss tensor 3 samples: {l1.item()}')\n",
    "print(f'L2 loss tensor 3 samples: {l2.item()}')\n",
    "\n",
    "# actual predictions\n",
    "_, predictions1 = torch.max(Y_pred_good, dim=1)\n",
    "_, predictions2 = torch.max(Y_pred_bad, dim=1)\n",
    "print(predictions1)\n",
    "print(predictions2)\n"
   ],
   "id": "8bfeef12ebd3753f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss tensor 0.4170299470424652\n",
      "L2 loss tensor 1.840616226196289\n",
      "tensor([0])\n",
      "tensor([1])\n",
      "L1 loss tensor 3 samples: 0.3018244206905365\n",
      "L2 loss tensor 3 samples: 1.6241613626480103\n",
      "tensor([2, 0, 1])\n",
      "tensor([0, 2, 1])\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T08:54:48.654903Z",
     "start_time": "2025-02-11T08:54:48.649627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "Multiclass cross entropy using a NN and torch.CrossEntropyLoss function\n",
    "'''\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "# Multiclass problem\n",
    "class NeuralNet2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet2, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, num_classes)\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x) # input layer\n",
    "        out = self.relu(out) # activation function \n",
    "        out = self.linear2(out) # last layer\n",
    "        # no softmax at the end\n",
    "        return out\n",
    "\n",
    "model = NeuralNet2(input_size=28*28, hidden_size=5, num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss() # applies softmax\n",
    "# missing parts: data, run the forward and then calculate the loss. \n",
    "# TODO: implement this shit \n"
   ],
   "id": "676522a2bfefc50b",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "'''\n",
    "Binary class Neural Network with cross entropy. We ask: is this a dog?\n",
    "For this example, we only have one output on our last layer and then apply the sigmoid(which will return a probability between 0-1 with <0.5=false, >0.5=true)\n",
    "In PyTorch: Use nn.BCELoss()\n",
    "Sigmoid at the end!\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Binary classification\n",
    "class NeuralNet1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size): # no need for num_classes since we only have one class\n",
    "        super(NeuralNet1, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 1) # only one output \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        #sigmoid at the end\n",
    "        y_pred = torch.sigmoid(out)\n",
    "        return y_pred\n",
    "    \n",
    "model = NeuralNet1(input_size=28*28, hidden_size=5)\n",
    "criterion = nn.BCELoss()\n",
    "# TODO: implement this shit "
   ],
   "id": "8ba79c21a8d659f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "ACTIVATION FUNCTIONS    ",
   "id": "fa680b014f99dce6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "''''\n",
    "Activation Functions:\n",
    "Activation functions apply a non-linear transformation and decide whether a neuron should be activated or not\n",
    "WHY?\n",
    "Without activation functions our network is basically just a stacked linear regression model. We use non-linear activation functions\n",
    "so that our model can learn more complex behaviors and perform better.\n",
    "--> With non-linear transformations our network can learn better and perform more complex tasks!\n",
    "--> After each layer we typically use an activation function!\n",
    "\n",
    "Most popular activation functions:\n",
    "1.- Step function: output 1 if outputs is greater than a threshold(theta) and 0 otherwise. Not use in practice\n",
    "2.- Sigmoid Function: Output a probability between 0 and 1. It is typically used in the last layer of a binary classification problem\n",
    "3.- TanH Function(Hyperbolic tan function): It is a scaled sigmoid function with some shift. It will output a value between -1 and +1. \n",
    "It is a good option for hidden layers\n",
    "4.- ReLU Function: Most popular activation function. The function will output a 0 for negative values and output a linear function for values greater than 0. It is actually non-linear (f(x) = max(0,x))\n",
    "Rule of thumb: If you don't know what to use, just use a ReLU for hidden layers\n",
    "5.- Leaky ReLU Function: For negative numbers, it will output the value times a small value \"a\", and the regular ReLU otherwise\n",
    "--> Improved version of ReLU. Tries to solve the vanishing gradient problem. With a normal ReLU, our values for negative inputs are 0 and this means that the gradient is also 0 during our backpropagation. When the gradient is 0, the weights will never be updated, so the neurons won't learn anything because they are dead. \n",
    "6.- SoftMax: Squashing function that forces the outputs to be probabilities between 0 and 1.  \n",
    "--> Good in last layer in multi class classification problems \n",
    "'''\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    " \n",
    "''''\n",
    "Activation functions in nn:\n",
    "- nn.Sigmoid\n",
    "- nn.TanH\n",
    "- nn.LeakyReLU\n",
    "- nn.Softmax\n",
    "\n",
    "Activation functions in torch:\n",
    "- torch.relu\n",
    "- torch.sigmoid\n",
    "- torch.softmax\n",
    "- torch.tanh\n",
    "\n",
    "Some activation functions are not available in the torch api directly, but they are available in the torch.nn.functional:\n",
    "- F.relu\n",
    "- F.leaky_relu\n",
    "'''\n",
    "# option 1 (create nn modules)\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "# option 2 (use activation functions directly in forward pass)\n",
    "class NeuralNet2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet2, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.linear1(x))\n",
    "        out = torch.sigmoid(self.linear2(out))\n",
    "        return out\n"
   ],
   "id": "6e0b8e4c90a6eb26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4cda4d9e2dd894fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b17f2545eab07a46"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "13a9bae2c0c096f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "32ef86e04539d5e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bb2d9ff75b991974"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9cc93659be767dad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4caa9dd186446883"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a37a3cb6f3d29b13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f8279eaac9e8b34b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6ac714482e91849b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3886228dc79f4cd9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cd9d2e82546ffbee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a02ee922fa90d0b6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
