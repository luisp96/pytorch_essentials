{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import torch\n",
    "from numpy import dtype\n",
    "from sympy.codegen.ast import float32\n",
    "from tensorboard.summary.v1 import image\n",
    "from torchvision.transforms.v2 import ToTensor\n",
    "\n",
    "x = torch.rand(3)\n",
    "print(x)\n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "x = torch.empty(3)\n",
    "print(x)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = torch.empty(2, 3)\n",
    "print(x)"
   ],
   "id": "ca7aaaf5d1731352",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = torch.empty(2, 3, 2)\n",
    "print(x)"
   ],
   "id": "25599daaede69323",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = torch.empty(2, 3, 2, 2)\n",
    "print(x)"
   ],
   "id": "503090d1d04da03a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = torch.rand(2,2)\n",
    "print(x)"
   ],
   "id": "30df6d240acb7a48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = torch.zeros(2,2)\n",
    "print(x)"
   ],
   "id": "68d786a4fe2ff35d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = torch.ones(2,2)\n",
    "print(x)"
   ],
   "id": "d5f3adff041544c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = torch.ones(2,2)\n",
    "print(x.dtype)"
   ],
   "id": "f4e1a70e2582fdc9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = torch.ones(2,2, dtype=torch.int) #torch.double, torch.float16, torch.float64\n",
    "print(x.dtype) #prints the dtype of the current tensor"
   ],
   "id": "f05cf91f7f4cd89b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = torch.ones(2,2, dtype=torch.float16)\n",
    "print(x.size()) # prints the size of the current tensor"
   ],
   "id": "bde25427470147e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = torch.tensor([2.5, 0.1])\n",
    "print(x)"
   ],
   "id": "fc07c30934b2dc32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "BASIC OPERATIONS",
   "id": "2556f15d79d1c98f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = torch.rand(2,2)\n",
    "y = torch.rand(2,2)\n",
    "print(x)\n",
    "print(y)\n",
    "z = x + y\n",
    "z = torch.add(x,y)\n",
    "print(z) # it does element wise addition\n"
   ],
   "id": "b9744c58621c7d24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y.add_(x) # inplace operation. Add all of the x's to the y. It modifies the variable in which it is applied to\n",
    "print(y)"
   ],
   "id": "e4eb8fca20db4f9d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "z = x - y\n",
    "z = torch.sub(x,y)\n",
    "print(z)"
   ],
   "id": "da9ab11c3c3b5236",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "z = x * y\n",
    "z = torch.mul(x,y)\n",
    "print(z)\n",
    "y.mul_(x) #inplace operation"
   ],
   "id": "89abdf50080bf0f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "z = x / y\n",
    "z = torch.div(x,y)"
   ],
   "id": "51256b0dee34b3cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#slciing operations\n",
    "x = torch.rand(5,3)\n",
    "print(x[:,0]) #slicing for all rows but only the first column of each row\n",
    "print(x[1, :]) #slicing for only the first row but all of the columns from said row\n",
    "#you can also get the value of a tensor, if and only if is a single element tensor\n",
    "print(x[1, 1].item())\n"
   ],
   "id": "d8b65094524e5c0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# reshaping a tensor\n",
    "x = torch.rand(4,4)\n",
    "print(x)\n",
    "y = x.view(16) #resizes the 16 values in a 4x4 tensor and a single 1x16. NOTE: the number of elements must match\n",
    "print(y)\n",
    "\n",
    "y = x.view(-1, 8) #if we don't want to resize to a single dimension tensor, then we can put a -1 on the first position and pytorch will correctly assume the dimension for the number of rows. i.e. 2x8\n",
    "print(y.size)"
   ],
   "id": "3b7a34c76b436bbb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#converting from numpy to tensor and viceversa\n",
    "import numpy as np\n",
    "# convert from a tensor into a np array\n",
    "a = torch.ones(5)\n",
    "print(a)\n",
    "b = a.numpy()\n",
    "print(type(b))  #i.e. numpy ndarray\n",
    "\n",
    "#if the tensor is in the cpu, then both objects(tensor and nparray) will share the same memory location, so any change to either will reflect on the other\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ],
   "id": "8d07a966d0a43a04",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#convert from a numpy array to a tensor\n",
    "a = np.ones(5)\n",
    "print(a)\n",
    "b = torch.from_numpy(a) #be careful changing the dtype, both must match. Otherwise you get an error\n",
    "print(b)\n",
    "\n",
    "a += 1\n",
    "print(a)\n",
    "print(b)\n"
   ],
   "id": "2ac57e2f9e1e065b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#gpu operations\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    x = torch.ones(5, device=device) #creates a tensor in the GPU \n",
    "    y = torch.ones(5) \n",
    "    y = y.to(device) # this moves the tensor to the GPU\n",
    "    z = x + y # this will be computed in the GPU\n",
    "    # you can't convert a GPU tensor into a numpy ndarray, so you must first move it into the cpu(numpy can only handle cpu tensors)\n",
    "    z = z.to(\"cpu\")\n",
    "    print(z)"
   ],
   "id": "b6fb73da74848bf7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "x = torch.ones(5, requires_grad=True) #the requires_grad flag is FALSE by default. It tells pytorch that it will need to calculate the gradients for this tensor later in the optimization steps. \n",
    "print(x) # this will also print the requires_grad flag as well"
   ],
   "id": "5f0ff1aa6e5b7e86",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Gradient Calculations with autograd",
   "id": "d9e7f31a380fc5d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#gradients are essential for optimization\n",
    "import torch \n",
    "\n",
    "x = torch.rand(3, requires_grad=True) # must specify the requires_grad to calculate the gradients\n",
    "print(x)\n",
    "y = x + 2 # pytorch creates a computational graph\n",
    "# pytorch will compute the forward pass, and create a grad_fn function and calculate the gradients during the backpropagation step(dy/dx)\n",
    "print(y) #has a grad_fn for AddBackward\n",
    "z = y*y*2\n",
    "print(z) #has a grad_fn for MulBackward\n",
    "z = z.mean()\n",
    "print(z) # has a grad_fn for MeanBackward\n",
    "\n",
    "# now when we calculate the gradients for z \n",
    "z.backward() #backward gradient calculation  dz/dx. The backward function will only work with scalar values(single value), if called without an argument\n",
    "print(x.grad) # the gradients are stored in x.grad\n",
    "#NOTE if we don't specify the requires_grad, then we won't have a .backward() grad_fn, giving us an error\n",
    " \n",
    "\n"
   ],
   "id": "91dbf909bca1c55b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The gradient calculation calculates a Jacobian product to get the gradients. We multiply the jacobian matrix containing the partial derivatives with a gradient vector(of the same size )",
   "id": "1494434079cb57b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.001], dtype=torch.float32)\n",
    "z.backward(v) #dz/dx\n",
    "print(x.grad)"
   ],
   "id": "270eecacae975a00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# sometimes during our training, when we want update the weight, we don't want pytorch to calculate the gradients\n",
    "# we have 3 options for this\n",
    "\n",
    "# x.requires_grad_(False)\n",
    "# x.detach()\n",
    "# with torch.no_grad()\n",
    "\n",
    "x.requires_grad_(False) # the underscore means that pytorch will modify the variable in place\n",
    "print(x)\n",
    "\n",
    "y = x.detach() # creates a new tensor with the same values but it doesn't requrie the gradients (so requires_grad = False)\n",
    "print(y)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = x + 2\n",
    "    print(z)"
   ],
   "id": "693ea07e74630dff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# VERY IMPORTANT: Whenever we call the backward function, then the gradient for the tensor will be accumulated into the .grad attribute, so the values will be summed up\n",
    "weights = torch.ones(4, requires_grad=True)\n",
    "for epoch in range(3):\n",
    "    model_output = (weights*3).sum()\n",
    "    \n",
    "    model_output.backward()\n",
    "    \n",
    "    print(weights.grad) #gradients get aggregated [[3...],[6....],[9.....]]\n",
    "    weights.grad.zero_() #you must empty the gradients after each iteration to get the correct gradients [3....]\n",
    "    \n",
    "# same thing but with an optimizer\n",
    "optimizer = torch.optim.SGD(weights, lr=0.01)\n",
    "optimizer.step() # next iteration\n",
    "optimizer.zero_grad() # empty the gradients "
   ],
   "id": "226ece1ae8d3919e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "BACKPROPAGATION - THEORY WITH EXAMPLES",
   "id": "1ea655f22661c3c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "#forward pass and compute the loss\n",
    "y_hat = w * x\n",
    "loss = (y_hat - y)**2\n",
    "print(loss)\n",
    "\n",
    "#backward pass\n",
    "loss.backward() #calculates the gradients during the backward pass and stores them in the tensors that have requires_grad \n",
    "print(w.grad) #prints the stored gradients for w\n",
    "\n",
    "##update weights\n",
    "## next forward and backward pass"
   ],
   "id": "bd08716d50bb05f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "GRADIENT DESCENT WITH AUTOGRAD AND BACKPROPAGATION",
   "id": "7917e2fd855354f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# f = w * x\n",
    "# f = 2 * x\n",
    "x = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
    "\n",
    "w = 0.0\n",
    "\n",
    "# calculate the model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "# calculate the loss = MSE\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted - y) ** 2).mean()\n",
    "# calculate the gradient\n",
    "# MSE = 1/N * (w*x - y)**2\n",
    "# dJ/dw = 1/N 2x (w*x - y)\n",
    "def gradient(x,y,y_predicted):\n",
    "    return np.dot(2*x, y_predicted-y).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward(x)\n",
    "    # loss\n",
    "    l = loss(y, y_pred)\n",
    "    # gradient\n",
    "    dw = gradient(x,y,y_pred)\n",
    "    # update the weights\n",
    "    w -= learning_rate*dw\n",
    "    if epoch % 2 == 0:\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ],
   "id": "9229fe8d3e756ec3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#gradient calculation with torch\n",
    "import torch\n",
    "\n",
    "# f = w * x\n",
    "# f = 2 * x\n",
    "x = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# calculate the model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "# calculate the loss = MSE\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted - y) ** 2).mean()\n",
    "# calculate the gradient\n",
    "# MSE = 1/N * (w*x - y)**2\n",
    "# dJ/dw = 1/N 2x (w*x - y)\n",
    "def gradient(x,y,y_predicted):\n",
    "    return np.dot(2*x, y_predicted-y).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward(x)\n",
    "    # loss\n",
    "    l = loss(y, y_pred)\n",
    "    # gradient = backward pass\n",
    "    l.backward() #dl/dw\n",
    "    # update the weights\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate*w.grad\n",
    "    \n",
    "    # zero gradients\n",
    "    w.grad.zero_()\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ],
   "id": "8a7959a25fc9ad57",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1) Design model (input, output size, forward pass)\n",
    "# 2) Construct the loss and optimizer\n",
    "# 3) Training loop \n",
    "#   - Forward pass: compute the prediction \n",
    "#   - Backward pass: compute the gradients\n",
    "#   - Update the weights \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# f = w * x\n",
    "# f = 2 * x\n",
    "x = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# calculate the model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w], lr=learning_rate)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward(x)\n",
    "    # loss\n",
    "    l = loss(y, y_pred)\n",
    "    # gradient = backward pass\n",
    "    l.backward() #dl/dw\n",
    "    # update the weights\n",
    "    optimizer.step()\n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "\n"
   ],
   "id": "191ff850a3228121",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1) Design model (input, output size, forward pass)\n",
    "# 2) Construct the loss and optimizer\n",
    "# 3) Training loop \n",
    "#   - Forward pass: compute the prediction \n",
    "#   - Backward pass: compute the gradients\n",
    "#   - Update the weights \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# f = w * x\n",
    "# f = 2 * x\n",
    "x = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "\n",
    "x_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = x.shape\n",
    "print(n_samples, n_features)\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "# model = nn.Linear(input_size, output_size)\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        #define layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "    \n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "print(f'Prediction before training: f(5) = {model(x_test).item():.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = model(x)\n",
    "    # loss\n",
    "    l = loss(y, y_pred)\n",
    "    # gradient = backward pass\n",
    "    l.backward() #dl/dw\n",
    "    # update the weights\n",
    "    optimizer.step()\n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        [w, b] = model.parameters()\n",
    "        print(f'epoch {epoch + 1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {model(x_test).item():.3f}')\n",
    "\n",
    "\n"
   ],
   "id": "575255e06b7f0894",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "LINEAR REGRESSION",
   "id": "6b21f56f5b27dd8c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1) Design model (input, output size, forward pass)\n",
    "# 2) Construct loss and optimizer\n",
    "# 3) Training loop\n",
    "#  - forward pass: compute prediction and loss\n",
    "#  - backward pass: gradients\n",
    "#  - update weights\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 0) Prepare data\n",
    "x_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=1)\n",
    "x = torch.from_numpy(x_numpy.astype(np.float32))\n",
    "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
    "# reshape the tensor to a column vector\n",
    "y = y.view(y.shape[0], 1)\n",
    "\n",
    "n_samples, n_features = x.shape\n",
    "# 1) model design \n",
    "input_size = n_features\n",
    "output_size = 1\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# 2) define loss and optimizer\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# 3) training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # forward pass and loss\n",
    "    y_predicted = model(x)\n",
    "    loss = criterion(y_predicted, y)\n",
    "    # backward pass (backpropagation) i.e. calculation of the derivatives\n",
    "    loss.backward()\n",
    "    # update the weights\n",
    "    optimizer.step() # updates the weights. The w is already in the optimizer, so no need to call w.grad to calculate the new weights\n",
    "    optimizer.zero_grad() # empty out the gradients before the next iterations. Otherwise, the backward function will sum up the gradients into the .grad attribute(.grad is internal to the optimizer in this case)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'epoch: {epoch + 1}, loss: {loss.item():.4f}')\n",
    "        \n",
    "#plot\n",
    "predicted = model(x).detach() # the detach disables the tensor from being tracked on the computational graph\n",
    "plt.plot(x_numpy, y_numpy, 'ro')\n",
    "plt.plot(x_numpy, predicted, 'b')\n",
    "plt.show()"
   ],
   "id": "81b40f50f3dc1c47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "LOGISTIC REGRESSION ",
   "id": "13072263aedc1028"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1) Design model (input, output size, forward pass)\n",
    "# 2) Construct loss and optimizer\n",
    "# 3) Training loop\n",
    "#  - forward pass: compute prediction and loss\n",
    "#  - backward pass: gradients\n",
    "#  - update weights\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 0) prepare data\n",
    "bc = datasets.load_breast_cancer()\n",
    "x, y = bc.data, bc.target\n",
    "\n",
    "n_samples, n_features = x.shape\n",
    "print(n_samples, n_features)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1234)\n",
    "# scale the features\n",
    "sc = StandardScaler() #when dealing with logistic, we want to scale our data to have zero mean and uniform variance \n",
    "X_train = sc.fit_transform(x_train) #what is the difference between fit_transform and just transform?\n",
    "X_test = sc.transform(x_test)\n",
    "\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32)) # make sure to cast as np.float32 because the original data is of type Double and will cause problems down the road\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "y_train = y_train.view(y_train.shape[0], 1) # what is this shape change?\n",
    "y_test = y_test.view(y_test.shape[0], 1)\n",
    "\n",
    "# 1) setup the model \n",
    "# f = wx + b, sigmoid function at the end \n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, n_input_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y_predicted = torch.sigmoid(self.linear(x))\n",
    "        return y_predicted\n",
    "    \n",
    "model = LogisticRegression(n_features)\n",
    "\n",
    "# 2) setup the loss and optimizer\n",
    "learning_rate = 0.01\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3) training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    #forward pass and loss calculation\n",
    "    y_predicted = model(X_train)\n",
    "    loss = criterion(y_predicted, y_train)\n",
    "    #backward pass\n",
    "    loss.backward()\n",
    "    #updates weights\n",
    "    optimizer.step()\n",
    "    #zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'epoch: {epoch + 1}, loss: {loss.item():.4f}')\n",
    "        \n",
    "with torch.no_grad(): # for evaluation, we don't want the gradient calculations to be tracked in the computational graph for (model), so we must use torch.no_grad()\n",
    "    y_predicted = model(X_test)\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'Accuracy: {acc:.4f}')"
   ],
   "id": "22b9c660ec167f00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "DATASET AND DATALOADER  ",
   "id": "f861a6800f088e89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "'''\n",
    "epoch = 1 forward and backward pass of ALL training samples\n",
    "batch_size = number of training samples in one forward & backward pass\n",
    "number of iterations = number of passes, each pass using [batch_size] number of samples\n",
    "e.g. 100 samples, batch_size=20 --> 100/20 = 5 iterations for 1 epoch\n",
    "'''\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class WineDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # data loading\n",
    "        xy = np.loadtxt('./wine.csv', delimiter=',', dtype=np.float32, skiprows=1)\n",
    "        self.x = torch.from_numpy(xy[:, 1:])\n",
    "        self.y = torch.from_numpy(xy[:, [0]]) # n_samples, 1\n",
    "        self.n_samples = xy.shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        # dataset[0] helps for indexing\n",
    "        return self.x[item], self.y[item]\n",
    "    def __len__(self):\n",
    "        # len(dataset)\n",
    "        return self.n_samples\n",
    "    \n",
    "dataset = WineDataset()\n",
    "# data iteration without dataloader\n",
    "# first_data = dataset[0]\n",
    "# features, labels = first_data\n",
    "# print(features, labels)\n",
    "\n",
    "# data iteration with dataloader\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=4, shuffle=True)\n",
    "# dataiter = iter(dataloader)\n",
    "# data = next(dataiter)\n",
    "# features, labels = data\n",
    "# print(features, labels)\n",
    "\n",
    "#training loop\n",
    "num_epochs = 2\n",
    "batch_size = 4\n",
    "total_samples = len(dataset)\n",
    "n_iterations = math.ceil(total_samples/batch_size)\n",
    "print(total_samples, n_iterations)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        # forward and backward pass, update the weights\n",
    "        if (i+5) % 5 ==0:\n",
    "            print(f'epoch: {epoch + 1}/{num_epochs}, step {i+1}/{n_iterations}, inputs {inputs.shape}')\n"
   ],
   "id": "c8ecf37ba894cf47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "DATASET TRANSFORM",
   "id": "9c4f78e242523917"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "'''\n",
    "Transforms can be applied to PIL images, tensors, ndarrays, or custom data during creation of the DataSet\n",
    "\n",
    "complete list of built-in transforms:\n",
    "https://pytorch.org/docs/stable/transforms.html\n",
    "\n",
    "On Images\n",
    "----------\n",
    "CenterCrop, Grayscale, Pad, RandomAffine\n",
    "RandomCrop, RandomHorizontalFlip, RandomVerticalFlip, \n",
    "RandomRotation, Resize, Scale\n",
    "\n",
    "On Tensors\n",
    "-----------\n",
    "LinearTransformation, Normalize, RandomErasing\n",
    "\n",
    "Conversion\n",
    "-----------\n",
    "ToPilImage: from tensor or ndarray\n",
    "ToTensor: from numpy.ndarray or PILImage\n",
    "\n",
    "Generic\n",
    "-----------\n",
    "Use Lambda\n",
    "\n",
    "Custom\n",
    "-----------\n",
    "Write own class\n",
    "\n",
    "Compose Multiple Transforms\n",
    "-----------------------------\n",
    "composed = transforms.Compose([Rescale(256),\n",
    "                                RandomCrop(224),])\n",
    "torchvision.transforms.ReScale(256)\n",
    "torchvision.transforms.ToTensor()\n",
    "\n",
    "dataset = torchvision.datasets.MNIST( root=\"./\", transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class WineDataset(Dataset):\n",
    "    def __init__(self, transform=None): # the = on the parameter indicates default value, which in this case would be None\n",
    "        # data loading\n",
    "        xy = np.loadtxt('./wine.csv', delimiter=',', dtype=np.float32, skiprows=1)\n",
    "        self.n_samples = xy.shape[0]\n",
    "        \n",
    "        #note that we do not convert to tensor here\n",
    "        self.x = xy[:, 1:]\n",
    "        self.y = xy[:, [0]]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        # dataset[0] helps for indexing\n",
    "        sample =  self.x[item], self.y[item]\n",
    "        # if there is a transform\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample) # applies the custom transform class. \n",
    "            \n",
    "        return sample  # return the sample, with a transform if one was specified\n",
    "            \n",
    "    def __len__(self):\n",
    "        # len(dataset)\n",
    "        return self.n_samples\n",
    "\n",
    "# custom transform class\n",
    "class ToTensor:\n",
    "    def __call__(self, sample): # the call function gets called automatically when we pass a sample into the ToTensor class as a function\n",
    "        inputs, targets = sample\n",
    "        return torch.from_numpy(inputs), torch.from_numpy(targets)\n",
    "    \n",
    "# a multiplication transform class\n",
    "class MulTransform:\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "    def __call__(self, sample):\n",
    "        inputs, targets = sample\n",
    "        inputs *= self.factor\n",
    "        return inputs, targets\n",
    "    \n",
    "dataset = WineDataset(transform=ToTensor()) \n",
    "first_data = dataset[0]\n",
    "features, labels = first_data\n",
    "print(features)\n",
    "print(type(features), type(labels)) # should be a torch.Tensor class\n",
    "\n",
    "composed = torchvision.transforms.Compose([ToTensor(), MulTransform(4)])\n",
    "dataset = WineDataset(transform=composed)\n",
    "first_data = dataset[0]\n",
    "features, labels = first_data\n",
    "print(features)\n",
    "print(type(features), type(labels)) # should be a torch.Tensor class\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "f5e6c143dd6e3110",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "SOFTMAX AND CROSS-ENTROPY FUNCTIONS   ",
   "id": "e218e1522de60a35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "'''\n",
    "The Softmax function squashes the raw outputs into probabilities i.e.[0 - 1]. It does this by applying the exponential function to each input\n",
    "and normalizing it by the sum of all inputs\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x), axis=0) # axis of x sums up the inputs across the columns, and y sums up the inputs across the rows.\n",
    "\n",
    "x = np.array([2.0,1.0, 0.1])\n",
    "outputs = softmax(x)\n",
    "print('softmax numpy:', outputs)\n",
    "\n",
    "# torch version of softmax\n",
    "x = torch.tensor([2.0, 1.0, 0.1])\n",
    "outputs = torch.softmax(x, dim=0) # dim is the same as axis for numpy\n",
    "print('softmax torch:', outputs)\n",
    "\n"
   ],
   "id": "7c32840319147ce4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "'''\n",
    "A lot times the softmax is combined with the cross-entropy loss function. This measures the performance of our classification model, whose output is a probability between \n",
    "0 and 1. It can be used in multi class problems. The loss increases as the predictor probability diverges from the actual label. So the better the prediction, the lower the loss. \n",
    "Note: in the case for probability label prediction, our label (Y), needs to be one-hot encoded i.e. Y = [1,0,0]. \n",
    "\n",
    "Y = [1, 0, 0] (Actual class label)\n",
    "(predicted class label from softmax)\n",
    "Y_hat = [0.7, 0.2, 0.1]  \n",
    "Cross_entropy = D(Y, Y_hat) = 0.35 (GOOD PREDICTION)\n",
    "\n",
    "Y = [1, 0, 0] (Actual class label)\n",
    "(predicted class label from softmax)\n",
    "Y_hat = [0.1, 0.3, 0.6]  \n",
    "Cross_entropy = D(Y, Y_hat) = 2.30 (GOOD PREDICTION)\n",
    "'''\n",
    "\n",
    "# Cross-Entropy code numpy\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def cross_entropy(actual, predicted):\n",
    "    loss = -np.sum(actual * np.log(predicted))\n",
    "    return loss # / float(predicted.shape[0]) The cross-entropy eqn dictates we normalize the output \n",
    "\n",
    "# y must be one hot encoded \n",
    "# if class 0: [1 0 0]\n",
    "# if class 1: [0 1 0]\n",
    "# if class 2: [0 0 1]\n",
    "Y = np.array([1, 0, 0])\n",
    "\n",
    "# y_pred has probabilities\n",
    "Y_pred_good = np.array([0.7,0.2,0.1])\n",
    "Y_pred_bad = np.array([0.1, 0.3, 0.6])\n",
    "l1 = cross_entropy(Y, Y_pred_good)\n",
    "l2 = cross_entropy(Y, Y_pred_bad)\n",
    "print(f'Loss1 numpy: {l1:.4f}')\n",
    "print(f'Loss2 numpy: {l2:.4f}')"
   ],
   "id": "fc99870bb1dfebc4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "'''\n",
    "Careful! \n",
    "nn.CrossEntropyLoss() applies:\n",
    "nn.LogSoftmax() + nn.NLLLoss() (negative log likelihood loss)\n",
    "\n",
    "--> No Softmax in last layer! i.e. we should not implement the softmax ourselves, since it is already applied\n",
    "\n",
    "Y has clas labels, NOT ONE-HOT!\n",
    "Y_pred has raw scores(logits), NO SOFTMAX!\n",
    "'''\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "Y = torch.tensor([0]) # remember it is not one hot encoded, but rather class label\n",
    "# n_samples * n_classes = 1x3\n",
    "Y_pred_good = torch.tensor([[2.0, 1.0, 0.1]]) # the class prediction has the raw values, no softmax i.e. normalization\n",
    "Y_pred_bad = torch.tensor([[0.5, 2.0, 0.3]])\n",
    "\n",
    "l1 = loss(Y_pred_good, Y) # here order matters, loss(prediction, actual)\n",
    "l2 = loss(Y_pred_bad, Y)\n",
    "\n",
    "print(f'L1 loss tensor {l1.item()}')\n",
    "print(f'L2 loss tensor {l2.item()}')\n",
    "\n",
    "# actual predictions\n",
    "_, predictions1 = torch.max(Y_pred_good, dim=1)\n",
    "_, predictions2 = torch.max(Y_pred_bad, dim=1)\n",
    "print(predictions1)\n",
    "print(predictions2)\n",
    "\n",
    "# loss function with multiple samples\n",
    "# 3 samples\n",
    "Y = torch.tensor([2, 0, 1])\n",
    "# n_samples * n_classes = 3x3\n",
    "Y_pred_good = torch.tensor([[0.1, 1.0, 2.1], [2.0, 1.0, 0.1], [0.1, 3.0, 0.1]]) # the class prediction has the raw values, no softmax i.e. normalization\n",
    "Y_pred_bad = torch.tensor([[2.1, 1.0, 0.1], [0.1, 1.0, 2.1], [0.1, 3.0, 0.1]])\n",
    "\n",
    "l1 = loss(Y_pred_good, Y) # here order matters, loss(prediction, actual)\n",
    "l2 = loss(Y_pred_bad, Y)\n",
    "\n",
    "print(f'L1 loss tensor 3 samples: {l1.item()}')\n",
    "print(f'L2 loss tensor 3 samples: {l2.item()}')\n",
    "\n",
    "# actual predictions\n",
    "_, predictions1 = torch.max(Y_pred_good, dim=1)\n",
    "_, predictions2 = torch.max(Y_pred_bad, dim=1)\n",
    "print(predictions1)\n",
    "print(predictions2)\n"
   ],
   "id": "8bfeef12ebd3753f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "'''\n",
    "Multiclass cross entropy using a NN and torch.CrossEntropyLoss function\n",
    "'''\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "# Multiclass problem\n",
    "class NeuralNet2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet2, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, num_classes)\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x) # input layer\n",
    "        out = self.relu(out) # activation function \n",
    "        out = self.linear2(out) # last layer\n",
    "        # no softmax at the end\n",
    "        return out\n",
    "\n",
    "model = NeuralNet2(input_size=28*28, hidden_size=5, num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss() # applies softmax\n",
    "# missing parts: data, run the forward and then calculate the loss. \n",
    "# TODO: implement this shit \n"
   ],
   "id": "676522a2bfefc50b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "'''\n",
    "Binary class Neural Network with cross entropy. We ask: is this a dog?\n",
    "For this example, we only have one output on our last layer and then apply the sigmoid(which will return a probability between 0-1 with <0.5=false, >0.5=true)\n",
    "In PyTorch: Use nn.BCELoss()\n",
    "Sigmoid at the end!\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Binary classification\n",
    "class NeuralNet1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size): # no need for num_classes since we only have one class\n",
    "        super(NeuralNet1, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 1) # only one output \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        #sigmoid at the end\n",
    "        y_pred = torch.sigmoid(out)\n",
    "        return y_pred\n",
    "    \n",
    "model = NeuralNet1(input_size=28*28, hidden_size=5)\n",
    "criterion = nn.BCELoss()\n",
    "# TODO: implement this shit "
   ],
   "id": "8ba79c21a8d659f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "ACTIVATION FUNCTIONS    ",
   "id": "fa680b014f99dce6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "''''\n",
    "Activation Functions:\n",
    "Activation functions apply a non-linear transformation and decide whether a neuron should be activated or not\n",
    "WHY?\n",
    "Without activation functions our network is basically just a stacked linear regression model. We use non-linear activation functions\n",
    "so that our model can learn more complex behaviors and perform better.\n",
    "--> With non-linear transformations our network can learn better and perform more complex tasks!\n",
    "--> After each layer we typically use an activation function!\n",
    "\n",
    "Most popular activation functions:\n",
    "1.- Step function: output 1 if outputs is greater than a threshold(theta) and 0 otherwise. Not use in practice\n",
    "2.- Sigmoid Function: Output a probability between 0 and 1. It is typically used in the last layer of a binary classification problem\n",
    "3.- TanH Function(Hyperbolic tan function): It is a scaled sigmoid function with some shift. It will output a value between -1 and +1. \n",
    "It is a good option for hidden layers\n",
    "4.- ReLU Function: Most popular activation function. The function will output a 0 for negative values and output a linear function for values greater than 0. It is actually non-linear (f(x) = max(0,x))\n",
    "Rule of thumb: If you don't know what to use, just use a ReLU for hidden layers\n",
    "5.- Leaky ReLU Function: For negative numbers, it will output the value times a small value \"a\", and the regular ReLU otherwise\n",
    "--> Improved version of ReLU. Tries to solve the vanishing gradient problem. With a normal ReLU, our values for negative inputs are 0 and this means that the gradient is also 0 during our backpropagation. When the gradient is 0, the weights will never be updated, so the neurons won't learn anything because they are dead. \n",
    "6.- SoftMax: Squashing function that forces the outputs to be probabilities between 0 and 1.  \n",
    "--> Good in last layer in multi class classification problems \n",
    "'''\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    " \n",
    "''''\n",
    "Activation functions in nn:\n",
    "- nn.Sigmoid\n",
    "- nn.TanH\n",
    "- nn.LeakyReLU\n",
    "- nn.Softmax\n",
    "\n",
    "Activation functions in torch:\n",
    "- torch.relu\n",
    "- torch.sigmoid\n",
    "- torch.softmax\n",
    "- torch.tanh\n",
    "\n",
    "Some activation functions are not available in the torch api directly, but they are available in the torch.nn.functional:\n",
    "- F.relu\n",
    "- F.leaky_relu\n",
    "'''\n",
    "# option 1 (create nn modules)\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "# option 2 (use activation functions directly in forward pass)\n",
    "class NeuralNet2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet2, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.linear1(x))\n",
    "        out = torch.sigmoid(self.linear2(out))\n",
    "        return out\n"
   ],
   "id": "6e0b8e4c90a6eb26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "FEED-FORWARD NEURAL NETWORK ",
   "id": "992080780f81bb25"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2025-02-13T06:25:06.323152Z",
     "start_time": "2025-02-13T06:23:48.694334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "Things included in this section:\n",
    "MNIST\n",
    "DataLoader, Transformation \n",
    "Multilayer Neural Net, activation function\n",
    "Loss and Optimizer\n",
    "Training Loop (batch training)\n",
    "Model Evaluation\n",
    "GPU support\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#hyper parameters\n",
    "input_size = 784 #28x28 images\n",
    "hidden_size = 100\n",
    "num_classes = 10\n",
    "num_epochs = 2\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST\n",
    "# parameters breakdown: root is the location to retrieve the dataset or save it into, train is to specify if this is the training data, transform is the transformations to be applied to the data\n",
    "# download is download if the dataset is not present on the root folder i.e. not already downloaded\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
    "# the shuffle true, just shuffles the train dataset for better randomness i guess\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "examples = iter(train_loader)\n",
    "samples, labels = next(examples)\n",
    "print(samples.shape, labels.shape)\n",
    "# torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
    "# 100 samples in our batch, 1 channel i.e. no color channels, 28x28 is the image size. The labels are 100\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.imshow(samples[i][0], cmap='gray') #cmap is color map\n",
    "#plt.show()\n",
    "\n",
    "# fully connected Neural Network with one hidden layer \n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, num_classes)\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out) #we don't apply softmax after this layer because we will use the cross-entropy function which already does this for us\n",
    "        return out\n",
    "    \n",
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device) #don't forget to send the model into the cuda device, as well as all the tensors that we will be using (images, labels)\n",
    "\n",
    "#loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#training loop \n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # 100, 1, 28, 28\n",
    "        # input_size = 784 so we need to resize our images\n",
    "        images = images.reshape(-1, 28*28).to(device) #pushing it to the gpu if one is available\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #forward\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        #backward \n",
    "        optimizer.zero_grad() #empty the values in the gradient\n",
    "        loss.backward() #calculate the gradients\n",
    "        optimizer.step() #update the weights\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'epoch {epoch + 1} / {num_epochs}, step {i+1}/{n_total_steps}, loss {loss.item():.4f}')\n",
    "\n",
    "# testing and evaluation\n",
    "with torch.no_grad(): # we don't want to calculate the gradients for the testing so we wrap it with torch.no_grad()\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # the max function will return the value and the index (value, index)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        n_samples += labels.shape[0] #should be 100\n",
    "        n_correct += (predictions == labels).sum().item() #for each correct prediction, we will add +1\n",
    "        \n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f\"accuracy = {acc:.2f}\")"
   ],
   "id": "4cda4d9e2dd894fd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "epoch 1 / 2, step 100/600, loss 0.4756\n",
      "epoch 1 / 2, step 200/600, loss 0.3629\n",
      "epoch 1 / 2, step 300/600, loss 0.2600\n",
      "epoch 1 / 2, step 400/600, loss 0.2948\n",
      "epoch 1 / 2, step 500/600, loss 0.1924\n",
      "epoch 1 / 2, step 600/600, loss 0.2959\n",
      "epoch 2 / 2, step 100/600, loss 0.1607\n",
      "epoch 2 / 2, step 200/600, loss 0.1504\n",
      "epoch 2 / 2, step 300/600, loss 0.2229\n",
      "epoch 2 / 2, step 400/600, loss 0.2383\n",
      "epoch 2 / 2, step 500/600, loss 0.0907\n",
      "epoch 2 / 2, step 600/600, loss 0.0749\n",
      "accuracy = 95.07\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Convolutional Neural Network    ",
   "id": "9e23412d2d815a48"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T09:44:22.538130Z",
     "start_time": "2025-02-13T09:42:25.217351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# we will be using the CIFAR-10 dataset available in pytorch to classify the images(which have 10 classes)\n",
    "'''\n",
    "Convolutional neural networks are mostly used for image classification. The typical architecture fo a CNN involves convolutions, activation functions and pooling\n",
    "The convolutional filters they work by applying a filter kernel in our image. The calculation is the multiplication of the original image region, with the filter and adding the result to be outputted to the resulting image(or output). With the transform, the resulting image output will be smaller, unless we use padding, or adjust the stride\n",
    "After conv filter application, we use the max pooling. In a 2x2 max pooling, we have a 2x2 region. We take the max value from output image, with respect to their pooling region and output that to \n",
    "the pooling output. This is done to reduce the computational cost by reducing the size of the image. It reduces the number of parameters our algorithm needs to learn. It also helps to avoid overfitting by providing an abstracted version of the image. \n",
    "'''\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# hyper-parameters\n",
    "num_epochs = 4\n",
    "batch_size = 4\n",
    "learning_rate = 0.001\n",
    "\n",
    "# dataset has PILImage(pillow images) of range[0, 1]\n",
    "# we transform them to tensors of normalized range [-1, 1]\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__() # don't forget to call the super\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5) #inputsize, outputsize, kernel size i.e. we have 3 color channels, 6 as our output and kernel size of 5\n",
    "        self.pool = nn.MaxPool2d(2, 2) # kernel size, stride size i.e. we want our downsized pooling section to be of a 2x2 region\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5) # the output of the first conv must match the input of this conv layer\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120) # \n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10) # our output size is 10 because we have 10 classes\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x))) # we apply the first conv layer, then the activation function(ReLu) and then we pool \n",
    "        x = self.pool(F.relu(self.conv2(x))) # same here\n",
    "        x = x.view(-1, 16 * 5 * 5) #We flatten our tensor. We put the -1 in so that pytorch can determine the proper size that should go there. In this case is the batch size. The second parameter is the combination of our last conv layer's output size times the kernel size of both layers i.e. 16*5*5\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) # no activation function at the end and no soft max since the cross entropy loss already calculates the soft max\n",
    "        return x\n",
    "        \n",
    "    \n",
    "model = ConvNet().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # this is the loss, and remember that the softmax is applied here so no need to implement one ourselves\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # origin shape: [4, 3, 32, 32] = 4, 3, 1024 \n",
    "        # input_layer: 3 input channels, 6 output channels, 5 kernel size\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad() # zero out the gradients so that they don't get added to each other\n",
    "        loss.backward() # actual propagation i.e. gradient calculation\n",
    "        optimizer.step() # update the weights\n",
    "        \n",
    "        if (i+1) % 2000 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "            \n",
    "print('Finished Training')\n",
    "\n",
    "# testing and validation\n",
    "with torch.no_grad(): # we surround the testing with torch.no_grad() because we don't need the gradient calculations \n",
    "    # stop = 0\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(10)]\n",
    "    n_class_samples = [0 for i in range(10)]\n",
    "    for images, labels in test_loader: # from the train loader, each loop gets #batch_size items\n",
    "        # so labels contains the labels for 4 items (in the current sequence) and outputs contains a 4-row tensor with 10 items on each row\n",
    "        # the torch.max for the outputs, gets the index of the highest probability class for each row and turns it into a 4 element tensor. \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # max returns (value, index)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        # the number of total samples in this evaluation is 10,000, 1,000 for each class \n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item() #sum returns the number of indices where pred==label aka #correct\n",
    "        # if stop < 5:\n",
    "        #     print(f'labels {labels}')\n",
    "        #     print(f'predicted {predicted}')\n",
    "        #     print(f\"sum {(predicted == labels).sum()} and item {(predicted == labels).sum().item()}\")\n",
    "        #     stop += 1\n",
    "        for i in range(batch_size):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f\"Accuracy of the network = {acc:.2f}%\")\n",
    "    # print(f\"Number of samples = {n_samples}, number of correct classification = {n_correct}\")\n",
    "    # print(f\"Number of class samples = {n_class_samples}\")\n",
    "    # print(f'Number of class correct = {n_class_correct}')\n",
    "    \n",
    "    for i in range(10):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {classes[i]}: {acc:.2f}%')"
   ],
   "id": "b17f2545eab07a46",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Epoch [1/4], Step [2000/12500], Loss: 2.2788\n",
      "Epoch [1/4], Step [4000/12500], Loss: 2.2868\n",
      "Epoch [1/4], Step [6000/12500], Loss: 2.2865\n",
      "Epoch [1/4], Step [8000/12500], Loss: 2.2905\n",
      "Epoch [1/4], Step [10000/12500], Loss: 2.3581\n",
      "Epoch [1/4], Step [12000/12500], Loss: 2.3622\n",
      "Epoch [2/4], Step [2000/12500], Loss: 1.9149\n",
      "Epoch [2/4], Step [4000/12500], Loss: 1.9585\n",
      "Epoch [2/4], Step [6000/12500], Loss: 1.7869\n",
      "Epoch [2/4], Step [8000/12500], Loss: 2.3626\n",
      "Epoch [2/4], Step [10000/12500], Loss: 1.8984\n",
      "Epoch [2/4], Step [12000/12500], Loss: 2.2473\n",
      "Epoch [3/4], Step [2000/12500], Loss: 1.7402\n",
      "Epoch [3/4], Step [4000/12500], Loss: 1.5221\n",
      "Epoch [3/4], Step [6000/12500], Loss: 1.0894\n",
      "Epoch [3/4], Step [8000/12500], Loss: 1.3691\n",
      "Epoch [3/4], Step [10000/12500], Loss: 2.2991\n",
      "Epoch [3/4], Step [12000/12500], Loss: 1.9329\n",
      "Epoch [4/4], Step [2000/12500], Loss: 1.9262\n",
      "Epoch [4/4], Step [4000/12500], Loss: 1.8236\n",
      "Epoch [4/4], Step [6000/12500], Loss: 1.7926\n",
      "Epoch [4/4], Step [8000/12500], Loss: 1.0861\n",
      "Epoch [4/4], Step [10000/12500], Loss: 2.0724\n",
      "Epoch [4/4], Step [12000/12500], Loss: 1.2337\n",
      "Finished Training\n",
      "labels tensor([3, 8, 8, 0], device='cuda:0')\n",
      "predicted tensor([3, 8, 8, 8], device='cuda:0')\n",
      "sum 3 and item 3\n",
      "labels tensor([6, 6, 1, 6], device='cuda:0')\n",
      "predicted tensor([6, 6, 1, 6], device='cuda:0')\n",
      "sum 4 and item 4\n",
      "labels tensor([3, 1, 0, 9], device='cuda:0')\n",
      "predicted tensor([5, 1, 8, 9], device='cuda:0')\n",
      "sum 2 and item 2\n",
      "labels tensor([5, 7, 9, 8], device='cuda:0')\n",
      "predicted tensor([5, 7, 9, 4], device='cuda:0')\n",
      "sum 3 and item 3\n",
      "labels tensor([5, 7, 8, 6], device='cuda:0')\n",
      "predicted tensor([5, 9, 8, 6], device='cuda:0')\n",
      "sum 3 and item 3\n",
      "Accuracy of the network = 45.17%\n",
      "Number of samples = 10000, number of correct classification = 4517\n",
      "Number of class samples = [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]\n",
      "Number of class correct = [426, 629, 199, 185, 485, 487, 606, 431, 638, 431]\n",
      "Accuracy of plane: 42.60%\n",
      "Accuracy of car: 62.90%\n",
      "Accuracy of bird: 19.90%\n",
      "Accuracy of cat: 18.50%\n",
      "Accuracy of deer: 48.50%\n",
      "Accuracy of dog: 48.70%\n",
      "Accuracy of frog: 60.60%\n",
      "Accuracy of horse: 43.10%\n",
      "Accuracy of ship: 63.80%\n",
      "Accuracy of truck: 43.10%\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#cnn test, this part showcases why the dimensions are what they are above\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision \n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Device configuration \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyper parameters\n",
    "num_epochs = 0\n",
    "batch_size = 4\n",
    "learning_rate = 0.001\n",
    "\n",
    "#dataset has PILImages of range [0, 1]\n",
    "# We transform them to Tensors of normalized range [-1, 1]\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "# get some random trining images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "#show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "conv1 = nn.Conv2d(3, 6, 5)\n",
    "pool = nn.MaxPool2d(2, 2)\n",
    "conv2 = nn.Conv2d(6, 16, 5)\n",
    "print(images.shape)\n",
    "x = conv1(images)\n",
    "print(x.shape)\n",
    "x = pool(x)\n",
    "print(x.shape)\n",
    "x = conv2(x)\n",
    "print(x.shape)\n",
    "x = pool(x)\n",
    "print(x.shape)"
   ],
   "id": "13a9bae2c0c096f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "TRANSFER LEARNING   ",
   "id": "2b57541fd8c3d14b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "'''\n",
    "Transfer learning is a machine learning method in which a model developed for a first task is then reused as the starting point for a model on a second task. \n",
    "For example, we can use a model that was trained to classify birds and cars and modify it a little bit in the last layer and then use it to classify bees and dogs. \n",
    "It allows rapid generation of new models(pre-trained) and it is super important, because training of a new model can be super time consuming. Training a model from scratch can take multiple days or even weeks. Transfer learning can achieve pretty good results, hence why it is so popular nowadays \n",
    "'''\n",
    "# we will use the resnet_18 CNN as our base model. This is a model that has been trained with more than 1 million images from the resnet image database. 18 layers deep, and can classify \n",
    "# objects into 1000 object categories. For our example, we only have 2 categories: bees and ants\n",
    "#ImageFolder, folder where the images from the dataset go\n",
    "#Scheduler, schedule the change of the learning rate\n",
    "#Transfer Learning\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "import os \n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "}\n",
    "\n",
    "#import data\n",
    "data_dir = './data/hymenoptera_data'\n",
    "sets = ['train','val']\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train','val']}\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=0) for x in ['train','val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train','val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "print(class_names)\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs-1}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train() # set model to training mode\n",
    "            else:\n",
    "                model.eval() # set model to evaluate mode\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            #Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                #forward\n",
    "                #track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    #backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        optimizer.zero_grad() # empty the gradients\n",
    "                        loss.backward() # calculate the gradients aka back propagate\n",
    "                        optimizer.step() # update weights\n",
    "                \n",
    "                #statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            \n",
    "            #deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "        print()\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in: {time_elapsed//60:.0f}m {time_elapsed%60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc*100:.4f}')\n",
    "    \n",
    "    #load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# finetuning the weights based on the new data\n",
    "model = models.resnet18(pretrained=True) # download the resnet18 model and pretrained=True, which already has the optimized weights\n",
    "# we want to exchange the last fully connected layer\n",
    "num_ftrs = model.fc.in_features # the number of input features for the last layer on the resnet model\n",
    "\n",
    "# create a new layer and assign it to the last layer \n",
    "model.fc = nn.Linear(num_ftrs, 2) # the outputs will be 2 because we only have two classes that we want to classify: bees and ants\n",
    "model.to(device) # move the model to the GPU, if one is available\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001) # we don't have to specify the momentum, its optional i.e. default value will be set\n",
    "\n",
    "# scheduler will update the learning rate\n",
    "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1) # we have to give it the optimizer, step size and gamma. This means that every 7 epochs, our learning rate is multiplied by the gamma\n",
    "\n",
    "# # loop over the epochs\n",
    "# for epoch in range(100):\n",
    "#     train() #optimizer.step()\n",
    "#     evaluate()\n",
    "#     scheduler.step()\n",
    "\n",
    "#call the training function\n",
    "model = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=20)\n",
    "\n",
    "#######################\n",
    "# Second option: freeze all the layers in the beginning and only train the very last layer. This is a faster option since the layers are frozen\n",
    "#######################\n",
    "model = models.resnet18(pretrained=True) # download the resnet18 model and pretrained=True, which already has the optimized weights\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False # freezes the beginning layers\n",
    "\n",
    "# we want to exchange the last fully connected layer and by default the requires_grad = True\n",
    "num_ftrs = model.fc.in_features # the number of input features for the last layer on the resnet model\n",
    "\n",
    "# create a new layer and assign it to the last layer \n",
    "model.fc = nn.Linear(num_ftrs, 2) # the outputs will be 2 because we only have two classes that we want to classify: bees and ants\n",
    "model.to(device) # move the model to the GPU, if one is available\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001) # we don't have to specify the momentum, its optional i.e. default value will be set\n",
    "\n",
    "# scheduler will update the learning rate\n",
    "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1) # we have to give it the optimizer, step size and gamma. This means that every 7 epochs, our learning rate is multiplied by the gamma\n",
    "\n",
    "#call the training function\n",
    "model = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=20)"
   ],
   "id": "32ef86e04539d5e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c28e9cc83704f106"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "TENSORBOARD",
   "id": "e3b5c5c652361885"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "'''\n",
    "Tensorboard is a visualization toolkit used to visualize and analyze our model and training pipeline. https://tensorflow.org/tensorboard\n",
    "- Tracking and visualizing metrics such as loss and accuracy\n",
    "- Visualizing the model graph (ops and layers)\n",
    "- Viewing histograms of weights, biases, or other tensors as they change over time\n",
    "- Projecting embeddings to a lower dimensional space\n",
    "- Displaying images, text and audio data\n",
    "- Profiling TensorFlow programs\n",
    "- And much more \n",
    "NOTE: For the pycharm jupyter notebook extension, make sure to run the tensorboard on the integrated terminal, not the anaconda environment terminal\n",
    "run: tensorboard --logdir=runs, which will open the tensorboard on http://localhost:6006\n",
    "\n",
    "WHAT IS A PRECISION/RECALL CURVE?\n",
    "Let's you understand your model performance and the different threshold settings. Also called, ROC curve\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "# setup the tensorboard writer\n",
    "from torch.utils.tensorboard import SummaryWriter # import the summary writer\n",
    "writer = SummaryWriter('runs')\n",
    "\n",
    "#device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#hyper parameters\n",
    "input_size = 784 #28x28 images\n",
    "hidden_size = 100\n",
    "num_classes = 10\n",
    "num_epochs = 2\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST\n",
    "# parameters breakdown: root is the location to retrieve the dataset or save it into, train is to specify if this is the training data, transform is the transformations to be applied to the data\n",
    "# download is download if the dataset is not present on the root folder i.e. not already downloaded\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
    "# the shuffle true, just shuffles the train dataset for better randomness i guess\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "examples = iter(train_loader)\n",
    "samples, labels = next(examples)\n",
    "print(samples.shape, labels.shape)\n",
    "# torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
    "# 100 samples in our batch, 1 channel i.e. no color channels, 28x28 is the image size. The labels are 100\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.imshow(samples[i][0], cmap='gray') #cmap is color map\n",
    "#plt.show()\n",
    "# plot to the tensorboard \n",
    "img_grid = torchvision.utils.make_grid(samples)\n",
    "writer.add_image(\"mnist_images\", img_grid)\n",
    "# writer.close() # all the outputs are being flushed\n",
    "# sys.exit() # exit because we don't want to run the whole training pipeline\n",
    "\n",
    "# fully connected Neural Network with one hidden layer \n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, num_classes)\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out) #we don't apply softmax after this layer because we will use the cross-entropy function which already does this for us\n",
    "        return out\n",
    "    \n",
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device) #don't forget to send the model into the cuda device, as well as all the tensors that we will be using (images, labels)\n",
    "\n",
    "#loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#add model graph to tensorboard\n",
    "writer.add_graph(model, samples.reshape(-1, 28*28).to(device)) #make sure to send the tensor(samples to the GPU), otherwise the graph won't show up on the tensorboard\n",
    "writer.close()\n",
    "# sys.exit()\n",
    "\n",
    "#training loop \n",
    "n_total_steps = len(train_loader)\n",
    "running_loss = 0.0\n",
    "running_correct = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # 100, 1, 28, 28\n",
    "        # input_size = 784 so we need to resize our images\n",
    "        images = images.reshape(-1, 28*28).to(device) #pushing it to the gpu if one is available\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #forward\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        #backward \n",
    "        optimizer.zero_grad() #empty the values in the gradient\n",
    "        loss.backward() #calculate the gradients\n",
    "        optimizer.step() #update the weights\n",
    "        \n",
    "        running_loss += loss.item() # calculate the running loss for the epoch\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1) # get the predictions\n",
    "        running_correct += (predicted == labels).sum().item() # compound the number of correct predictions for the epoch\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'epoch {epoch + 1} / {num_epochs}, step {i+1}/{n_total_steps}, loss {loss.item():.4f}')\n",
    "            writer.add_scalar('training loss', running_loss / 100, epoch * n_total_steps + i) # add the running training loss for the epoch to the tensorboard with a global step\n",
    "            writer.add_scalar('Accuracy', running_correct / 100, epoch * n_total_steps + i) # add the running correct predictions for the epoch to the tensorboard with a global step\n",
    "            running_loss = 0.0 # zero out the running loss for the next epoch iteration\n",
    "            running_correct = 0 # zero out the running correct predictions for the next epoch\n",
    "\n",
    "# testing and evaluation\n",
    "# In test phase, we don't need to compute gradients (For memory efficiency)\n",
    "####################\n",
    "#Adding Precision Curves\n",
    "####################\n",
    "labels_to_plot = []\n",
    "preds = []\n",
    "\n",
    "with torch.no_grad(): # we don't want to calculate the gradients for the testing so we wrap it with torch.no_grad()\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # the max function will return the value and the index (value, index)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        n_samples += labels.shape[0] #should be 100\n",
    "        n_correct += (predictions == labels).sum().item() #for each correct prediction, we will add +1\n",
    "        \n",
    "        # for PR curves\n",
    "        labels_to_plot.append(labels) #we append the labels to be plotted to our tensorboard as our ground truth\n",
    "           # we need our predictions to be represented as a probability, so we must apply the SoftMax to our outputs(during this evaluation phase)\n",
    "        class_predictions = [F.softmax(output, dim=0) for output in outputs]\n",
    "        preds.append(class_predictions)\n",
    "    \n",
    "    #still on the tensorboard side\n",
    "    preds = torch.cat([torch.stack(batch) for batch in preds]) # for our predictions we want to stack the predictions into a 2-dimensional tensor. We do this by concatenating \n",
    "    labels_to_plot = torch.cat(labels_to_plot) # converts the labels into a one-dimensional tensor\n",
    "        \n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f\"accuracy = {acc:.2f}\")\n",
    "    \n",
    "    # tensor board pr curve calculations\n",
    "    classes = range(10) # since we are doing digits for our classes, a simple range will suffice\n",
    "    for i in classes:\n",
    "        labels_i = labels_to_plot == i # we get the labels_i when labels_to_plot is i\n",
    "        preds_i = preds[:, i] # all the samples, but only for the class i\n",
    "        writer.add_pr_curve(str(i), labels_i, preds_i, global_step=0)\n",
    "        writer.close()"
   ],
   "id": "8a7f0c0f9b1a5fd4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "SAVING AND LOADING MODULES",
   "id": "7f0843c637c1c532"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "'''\n",
    "Methods that we need to save and load models \n",
    "torch.save(arg, PATH): For COMPLETE MODEL\n",
    "Can use tensors, models or any dictionary as parameter for saving. Saves it as a serialized pkl file, not human readable \n",
    "\n",
    "torch.load(PATH):\n",
    "\n",
    "model.load_state_dict(arg):\n",
    "\n",
    "OPTION 1: LAZY OPTION Disadvantage: the serialized data is bound to the specific classes and the exact directory structure used when the model is saved\n",
    "torch.save(model, PATH)  for saving\n",
    "For Retrieval\n",
    "model = torch.load(PATH)\n",
    "model.eval() this puts the model on the eval mode\n",
    "\n",
    "OPTION 2: THE RECOMMENDED WAY. If we just want to save our trained model and use it later for inference, then we only need to save the parameters\n",
    "### STATE DICT #####\n",
    "torch.save(model.state_dict(), PATH)  this saves the parameters when called on the model.state_dict()\n",
    "\n",
    "# for retrieval, model must be created again with parameters\n",
    "model = Model(*args, **kwargs)  this implies recreating the model, but do we need to recreate the structure as well? like n-hidden layers with the correct number of nodes on each layer? FOOD FOR THOUGHT\n",
    "model.load_state_dict(torch.load(PATH))  # we need to pass the PATH onto the torch.load() to properly load the dictionary \n",
    "model.eval()    sets the model to eval mode\n",
    "'''\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_input_features):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear1 = nn.Linear(n_input_features, 1) # this is a dummy model, with only one layer and mapped to one output\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y_pred = torch.sigmoid(self.linear1(x))\n",
    "        return y_pred\n",
    "    \n",
    "model = Model(n_input_features=6) # comment this out when loading the model\n",
    "for param in model.parameters(): # print the parameters of our original model. Since we don't do any training, our model is initialized with some random parameters\n",
    "    print(param)\n",
    "# train your model aka the training loop would go here\n",
    "\n",
    "'''\n",
    "THE LAZY METHOD OF SAVING THE WHOLE MODEL AND SUBSEQUENTLY LOADING IT\n",
    "'''\n",
    "# FILE = \"model.pth\" # the convention is to name our saved model as model.pth\n",
    "# # torch.save(model, FILE) # saves the entire model   (comment this out when loading the model)\n",
    "# #################loading the model##################\n",
    "# model = torch.load(FILE) # load the complete model (comment out when saving the model)\n",
    "# model.eval() # put the loaded model on eval mode (comment out when saving the model)\n",
    "# # we can inspect the parameters of our loaded model\n",
    "# for param in model.parameters():\n",
    "#     print(param)\n",
    "\n",
    "'''\n",
    "THE PREFERRED METHOD OF SAVING YOUR MODEL AND SUBSEQUENTLY LOADING IT\n",
    "'''\n",
    "FILE = \"model.pth\"\n",
    "torch.save(model.state_dict(), FILE) # here we are saving the model parameters, not the whole model (comment our when loading your model )\n",
    "######################LOADING THE MODEL PARAMETERS############################\n",
    "loaded_model = Model(n_input_features=6) # same structure as the original model(the saved one). We need to reconstruct the structure of the model. (Comment out when saving your model)\n",
    "loaded_model.load_state_dict(torch.load(FILE))\n",
    "loaded_model.eval()\n",
    "for param in loaded_model.parameters():\n",
    "    print(param)\n",
    "\n"
   ],
   "id": "bb2d9ff75b991974",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "'''\n",
    "How to save model checkpoints during training \n",
    "'''\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_input_features):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear1 = nn.Linear(n_input_features, 1) # this is a dummy model, with only one layer and mapped to one output\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y_pred = torch.sigmoid(self.linear1(x))\n",
    "        return y_pred\n",
    "    \n",
    "model = Model(n_input_features=6) # comment this out when loading the model\n",
    "# train your model....\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "print(model.state_dict()) \n",
    "print(optimizer.state_dict()) # the optimizer also has a state dict that contains things like lr, momentum, dampening, weight_decay, etc\n",
    "# let's say we want to stop training at a certain point and save the checkpoint\n",
    "# we first need to create a dictionary for our checkpoint\n",
    "checkpoint = {\n",
    "    'epoch': 90,\n",
    "    'model_state': model.state_dict(),\n",
    "    'optim_state': optimizer.state_dict()\n",
    "}\n",
    "# save the checkpoint\n",
    "# torch.save(checkpoint, \"checkpoint.pth\") # since torch.save() can save any dictionary, this is a valid way of saving the checkpoint (comment during checkpoint load)\n",
    "##############LAOD THE CHECKPOINT################\n",
    "loaded_checkpoint = torch.load(\"checkpoint.pth\")\n",
    "epoch = loaded_checkpoint['epoch']\n",
    "# recreate the model\n",
    "model = Model(n_input_features=6)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0)\n",
    "#load the state_dicts\n",
    "model.load_state_dict(loaded_checkpoint['model_state']) # loads the saved model's parameters\n",
    "optimizer.load_state_dict(loaded_checkpoint['optim_state']) #laods the saved optimizer's parameters\n",
    "####NOTE: Since we are intending on continuing training, we don't set the model to the eval mode\n",
    "print(optimizer.state_dict())"
   ],
   "id": "9cc93659be767dad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "'''\n",
    "LOADING AND SAVING TO AND FROM THE GPU/CPU\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "PATH = \"somepath\"\n",
    "\n",
    "#Save on GPU, Load on CPU\n",
    "device = torch.device(\"cuda\") #set to use the GPU\n",
    "model.to(device) # send the model to the GPU\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "device = torch.device(\"cpu\") # change to use the CPU instead\n",
    "model = Model(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH, map_location=device)) #herewe specify that we want to load our model onto our device, which is CPU at the moment\n",
    "\n",
    "\n",
    "#Save on GPU, Load on GPU\n",
    "device = torch.device(\"cuda\") # set your device to cuda for GPU\n",
    "model.to(device) # send the model to the GPU\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "model = Model(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.to(device) # send the loaded model to the GPU \n",
    "\n",
    "\n",
    "#Save on CPU, load on GPU\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = Model(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH, map_location=\"cuda:0\")) # Choose whatever GPU device, if you have more than 1. i.e. the number 0 is the GPU device\n",
    "model.to(device)\n",
    "# When you continue training, make sure to send the samples, i.e. data tensors to the same device to which you sent your model to "
   ],
   "id": "4caa9dd186446883",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Create and Deploy a Deep Learning APP - Pytorch Model Deployment With Flask & Heroku    ",
   "id": "d0b3d72df0bcbfbf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a37a3cb6f3d29b13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f8279eaac9e8b34b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6ac714482e91849b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3886228dc79f4cd9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "cd9d2e82546ffbee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a02ee922fa90d0b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7c40fa5eb149add1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1f0ebf49d12ce0da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f9e7abaefd2bc643",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "df04bfdac6856958",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "cb3e936b4e99c604",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
